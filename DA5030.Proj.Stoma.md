Full data set creation:
=======================

The first dataset contains survee results from blood surbey from 2007 -
2008 - specifically, examination of blood pressure. Blood pressure
values ( systolic , diastolic, enhacement used), as well as cuff size,
pulse rate, coffee, alcohol, food intake in the last 30 minutes were
recorded. The second dataset contains blood lab results - % of
basophils, monocytes, Segmented neutrophils percent, Eosinophils
percent, red blood cell count, and other standard and derived lab
values. This a valuable dataset for overall health evalutaion.

    # blood pressure data
    blood.2<- read.csv("25505-0011-Data.tsv", sep = "\t")
    blood.2<- blood.2[, 1:27]

    # blood lab work data 
    labs<- read.csv("25505-0102-Data.tsv", sep = "\t")
    labs<- labs[,1:21]

Third dataset contains reported health status for the patients in the
previosu two datasets. I want to examine whether it is sufficient
information to make predictions of the health status of a person based
on mostly blood related data. While blood labs and surveys have been a
cornerstone and a first step in many medical encounters, it is
interesting to see how that related to the overall reported health
status and weather general blood lab can be used to manage health status
**preemptively**.

    health<- read.csv("25505-0207-Data.tsv", sep = "\t")
    # we only want the response variable and SEQN to add it to the combined dataset:
    health<- health %>% dplyr::select(SEQN, HSD010)

Modify the features that need to be encoded as factors.

    blood.2$PEASCST1<- as.factor(blood.2$PEASCST1)
    blood.2$PEASCCT1<- as.factor(blood.2$PEASCCT1)
    blood.2$BPQ150A<- as.factor(blood.2$BPQ150A)
    blood.2$BPQ150B<- as.factor(blood.2$BPQ150B)
    blood.2$BPQ150C<- as.factor(blood.2$BPQ150C)
    blood.2$BPQ150D<- as.factor(blood.2$BPQ150D)
    blood.2$BPAARM<- as.factor(blood.2$BPAARM)
    blood.2$BPACSZ<- as.factor(blood.2$BPACSZ)
    blood.2$BPXPULS<- as.factor(blood.2$BPXPULS)
    blood.2$BPXPTY<- as.factor(blood.2$BPXPTY)
    blood.2$BPAEN1<- as.factor(blood.2$BPAEN1)
    blood.2$BPAEN2<- as.factor(blood.2$BPAEN2)
    blood.2$BPAEN3<- as.factor(blood.2$BPAEN3)
    blood.2$BPAEN4<- as.factor(blood.2$BPAEN4)

Combine the datasets based on the individual SEQN - patient ID.

    full.df<- inner_join(blood.2, labs, by = "SEQN")
    full.df<- inner_join(health, full.df, by = "SEQN")

    summary(full.df)

    ##       SEQN           HSD010      PEASCST1    PEASCTM1         PEASCCT1   
    ##  Min.   :41475   Min.   :1.000   1:8903   Min.   :   3.0   56     : 333  
    ##  1st Qu.:44015   1st Qu.:2.000   2:  26   1st Qu.: 473.0   2      :  23  
    ##  Median :46537   Median :3.000   3: 378   Median : 542.0   99     :  18  
    ##  Mean   :46551   Mean   :2.779            Mean   : 511.1   3      :  10  
    ##  3rd Qu.:49100   3rd Qu.:3.000            3rd Qu.: 629.0   7      :   8  
    ##  Max.   :51623   Max.   :9.000            Max.   :1521.0   (Other):  12  
    ##                  NA's   :2951             NA's   :392      NA's   :8903  
    ##      BPXCHR      BPQ150A     BPQ150B     BPQ150C     BPQ150D      BPAARM    
    ##  Min.   :  0.0   1   :1679   1   :   3   1   :  49   1   : 140   1   :7326  
    ##  1st Qu.: 90.0   2   :5691   2   :7367   2   :7321   2   :7230   2   :  36  
    ##  Median :100.0   NA's:1937   NA's:1937   NA's:1937   NA's:1937   8   :   8  
    ##  Mean   :100.3                                                   NA's:1937  
    ##  3rd Qu.:112.0                                                              
    ##  Max.   :184.0                                                              
    ##  NA's   :7746                                                               
    ##   BPACSZ         BPXPLS       BPXPULS      BPXPTY         BPXML1     
    ##  2   : 448   Min.   : 40.00   1   :8760   1   :7365   Min.   :110.0  
    ##  3   :2388   1st Qu.: 66.00   2   : 171   2   :   5   1st Qu.:130.0  
    ##  4   :3530   Median : 74.00   NA's: 376   NA's:1937   Median :140.0  
    ##  5   : 987   Mean   : 74.89                           Mean   :146.2  
    ##  NA's:1954   3rd Qu.: 82.00                           3rd Qu.:160.0  
    ##              Max.   :224.00                           Max.   :240.0  
    ##              NA's   :1946                             NA's   :1955   
    ##      BPXSY1          BPXDI1        BPAEN1         BPXSY2          BPXDI2      
    ##  Min.   : 74.0   Min.   :  0.00   2   :7354   Min.   : 76.0   Min.   :  0.00  
    ##  1st Qu.:108.0   1st Qu.: 58.00   NA's:1953   1st Qu.:108.0   1st Qu.: 59.50  
    ##  Median :118.0   Median : 68.00               Median :116.0   Median : 68.00  
    ##  Mean   :121.2   Mean   : 66.96               Mean   :120.1   Mean   : 66.96  
    ##  3rd Qu.:130.0   3rd Qu.: 76.00               3rd Qu.:130.0   3rd Qu.: 76.00  
    ##  Max.   :230.0   Max.   :116.00               Max.   :220.0   Max.   :122.00  
    ##  NA's   :2160    NA's   :2160                 NA's   :2431    NA's   :2431    
    ##   BPAEN2         BPXSY3          BPXDI3        BPAEN3         BPXSY4     
    ##  1   :  72   Min.   : 74.0   Min.   :  0.00   1   :  64   Min.   : 80.0  
    ##  2   :7282   1st Qu.:106.0   1st Qu.: 58.00   2   :7290   1st Qu.:110.0  
    ##  NA's:1953   Median :116.0   Median : 68.00   NA's:1953   Median :124.0  
    ##              Mean   :119.1   Mean   : 66.72               Mean   :127.7  
    ##              3rd Qu.:128.0   3rd Qu.: 76.00               3rd Qu.:140.0  
    ##              Max.   :222.0   Max.   :120.00               Max.   :214.0  
    ##              NA's   :2548    NA's   :2548                 NA's   :9082   
    ##      BPXDI4        BPAEN4        LBXWBCSI         LBXLYPCT       LBXMOPCT     
    ##  Min.   :  0.00   1   :  16   Min.   : 1.500   Min.   : 4.9   Min.   : 0.700  
    ##  1st Qu.: 62.00   2   : 784   1st Qu.: 5.800   1st Qu.:26.4   1st Qu.: 6.400  
    ##  Median : 72.00   NA's:8507   Median : 7.000   Median :32.5   Median : 7.800  
    ##  Mean   : 71.02               Mean   : 7.311   Mean   :33.7   Mean   : 7.999  
    ##  3rd Qu.: 82.00               3rd Qu.: 8.475   3rd Qu.:39.8   3rd Qu.: 9.300  
    ##  Max.   :118.00               Max.   :83.200   Max.   :85.5   Max.   :44.500  
    ##  NA's   :9082                 NA's   :1041     NA's   :1057   NA's   :1057    
    ##     LBXNEPCT        LBXEOPCT         LBXBAPCT          LBDLYMNO     
    ##  Min.   : 5.30   Min.   : 0.000   Min.   : 0.0000   Min.   : 0.400  
    ##  1st Qu.:47.60   1st Qu.: 1.600   1st Qu.: 0.4000   1st Qu.: 1.800  
    ##  Median :55.70   Median : 2.400   Median : 0.6000   Median : 2.200  
    ##  Mean   :54.59   Mean   : 3.063   Mean   : 0.6878   Mean   : 2.419  
    ##  3rd Qu.:62.70   3rd Qu.: 3.800   3rd Qu.: 0.8000   3rd Qu.: 2.800  
    ##  Max.   :92.10   Max.   :28.500   Max.   :15.9000   Max.   :71.100  
    ##  NA's   :1057    NA's   :1057     NA's   :1057      NA's   :1058    
    ##     LBDMONO          LBDNENO          LBDEONO          LBDBANO      
    ##  Min.   :0.1000   Min.   : 0.200   Min.   :0.0000   Min.   :0.0000  
    ##  1st Qu.:0.4000   1st Qu.: 2.800   1st Qu.:0.1000   1st Qu.:0.0000  
    ##  Median :0.5000   Median : 3.800   Median :0.2000   Median :0.0000  
    ##  Mean   :0.5668   Mean   : 4.059   Mean   :0.2197   Mean   :0.0414  
    ##  3rd Qu.:0.7000   3rd Qu.: 4.900   3rd Qu.:0.3000   3rd Qu.:0.1000  
    ##  Max.   :5.5000   Max.   :20.500   Max.   :2.9000   Max.   :2.1000  
    ##  NA's   :1058     NA's   :1058     NA's   :1058     NA's   :1058    
    ##     LBXRBCSI         LBXHGB         LBXHCT         LBXMCVSI     
    ##  Min.   :2.490   Min.   : 7.5   Min.   :22.40   Min.   : 55.90  
    ##  1st Qu.:4.360   1st Qu.:12.9   1st Qu.:37.30   1st Qu.: 83.10  
    ##  Median :4.650   Median :13.8   Median :40.20   Median : 86.90  
    ##  Mean   :4.672   Mean   :13.9   Mean   :40.35   Mean   : 86.55  
    ##  3rd Qu.:4.990   3rd Qu.:15.0   3rd Qu.:43.50   3rd Qu.: 90.60  
    ##  Max.   :6.970   Max.   :19.7   Max.   :56.30   Max.   :125.30  
    ##  NA's   :1040    NA's   :1040   NA's   :1040    NA's   :1040    
    ##     LBXMCHSI         LBXMC           LBXRDW        LBXPLTSI       LBXMPSI      
    ##  Min.   :18.00   Min.   :30.00   Min.   : 6.3   Min.   :  28   Min.   : 5.100  
    ##  1st Qu.:28.60   1st Qu.:33.80   1st Qu.:12.1   1st Qu.: 232   1st Qu.: 7.100  
    ##  Median :30.00   Median :34.50   Median :12.5   Median : 273   Median : 7.600  
    ##  Mean   :29.82   Mean   :34.43   Mean   :12.8   Mean   : 283   Mean   : 7.641  
    ##  3rd Qu.:31.30   3rd Qu.:35.10   3rd Qu.:13.1   3rd Qu.: 324   3rd Qu.: 8.100  
    ##  Max.   :60.80   Max.   :42.40   Max.   :37.8   Max.   :1000   Max.   :12.600  
    ##  NA's   :1040    NA's   :1040    NA's   :1040   NA's   :1040   NA's   :1040

Missing value exploration:
--------------------------

This graph does a quick summary of vizualization of missingness of the
data per column (% of all rows). These graphs are aids in feature
selection and decision making regarding eliminating entire features.

    library(naniar)
    gg_miss_var(full.df, show_pct = TRUE) + theme(axis.text.y = element_text(size = 6))

![](DA5030.Proj.Stoma_files/figure-markdown_strict/VIM%20full-1.png)

Further visualization of missingness:

    library(VIM)

    ## Loading required package: colorspace

    ## 
    ## Attaching package: 'colorspace'

    ## The following object is masked from 'package:pROC':
    ## 
    ##     coords

    ## Loading required package: grid

    ## VIM is ready to use.

    ## Suggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues

    ## 
    ## Attaching package: 'VIM'

    ## The following object is masked from 'package:datasets':
    ## 
    ##     sleep

    res<-summary(aggr(full.df, sortVar=TRUE))$combinations

![](DA5030.Proj.Stoma_files/figure-markdown_strict/VIM%20viz2%20full-1.png)

    ## 
    ##  Variables sorted by number of missings: 
    ##  Variable      Count
    ##    BPXSY4 0.97582465
    ##    BPXDI4 0.97582465
    ##  PEASCCT1 0.95659181
    ##    BPAEN4 0.91404319
    ##    BPXCHR 0.83227678
    ##    HSD010 0.31707317
    ##    BPXSY3 0.27377243
    ##    BPXDI3 0.27377243
    ##    BPXSY2 0.26120125
    ##    BPXDI2 0.26120125
    ##    BPXSY1 0.23208338
    ##    BPXDI1 0.23208338
    ##    BPXML1 0.21005695
    ##    BPACSZ 0.20994950
    ##    BPAEN1 0.20984205
    ##    BPAEN2 0.20984205
    ##    BPAEN3 0.20984205
    ##    BPXPLS 0.20908993
    ##   BPQ150A 0.20812292
    ##   BPQ150B 0.20812292
    ##   BPQ150C 0.20812292
    ##   BPQ150D 0.20812292
    ##    BPAARM 0.20812292
    ##    BPXPTY 0.20812292
    ##  LBDLYMNO 0.11367788
    ##   LBDMONO 0.11367788
    ##   LBDNENO 0.11367788
    ##   LBDEONO 0.11367788
    ##   LBDBANO 0.11367788
    ##  LBXLYPCT 0.11357043
    ##  LBXMOPCT 0.11357043
    ##  LBXNEPCT 0.11357043
    ##  LBXEOPCT 0.11357043
    ##  LBXBAPCT 0.11357043
    ##  LBXWBCSI 0.11185129
    ##  LBXRBCSI 0.11174385
    ##    LBXHGB 0.11174385
    ##    LBXHCT 0.11174385
    ##  LBXMCVSI 0.11174385
    ##  LBXMCHSI 0.11174385
    ##     LBXMC 0.11174385
    ##    LBXRDW 0.11174385
    ##  LBXPLTSI 0.11174385
    ##   LBXMPSI 0.11174385
    ##  PEASCTM1 0.04211884
    ##   BPXPULS 0.04039970
    ##      SEQN 0.00000000
    ##  PEASCST1 0.00000000

Feature selection:
------------------

Some values have the majority of the rows missing, so those features
will be omitted form the dataset overall: PEASCCT1 BPXCHR , BPXSY4
BPXDI4 BPAEN4 ;

There are consistently higher number of missing values, especially as we
are progression from the first measure of blood pressure towards 4th.
For the purpsoe of dealing with missing values in the ~10,000 members, I
am dropping the last 4 columns, which correspond to the measurement of
the fourth reading of systtolc and dystolic blood pressure in the blood
pressure data set. Since there are 9762 observations, having columns
with 9537 missing values of BP reading is useless, nor informative, and
bias prone if the values were to be imputed. Along the same line of
thinking, I am going to eliminate the feautre PEASCCT1 - blood pressure
comment. While it is a categorical feature, its input is not demanded
and is missing the good chunk of the data 9357 out of 9762. Similar for
BPXCHR.

    full.df<- full.df %>% 
      dplyr::select(-c(PEASCCT1, BPXSY4, BPXSY4, BPXDI4, BPAEN4, BPXCHR))
    summary(full.df)

    ##       SEQN           HSD010      PEASCST1    PEASCTM1      BPQ150A    
    ##  Min.   :41475   Min.   :1.000   1:8903   Min.   :   3.0   1   :1679  
    ##  1st Qu.:44015   1st Qu.:2.000   2:  26   1st Qu.: 473.0   2   :5691  
    ##  Median :46537   Median :3.000   3: 378   Median : 542.0   NA's:1937  
    ##  Mean   :46551   Mean   :2.779            Mean   : 511.1              
    ##  3rd Qu.:49100   3rd Qu.:3.000            3rd Qu.: 629.0              
    ##  Max.   :51623   Max.   :9.000            Max.   :1521.0              
    ##                  NA's   :2951             NA's   :392                 
    ##  BPQ150B     BPQ150C     BPQ150D      BPAARM      BPACSZ         BPXPLS      
    ##  1   :   3   1   :  49   1   : 140   1   :7326   2   : 448   Min.   : 40.00  
    ##  2   :7367   2   :7321   2   :7230   2   :  36   3   :2388   1st Qu.: 66.00  
    ##  NA's:1937   NA's:1937   NA's:1937   8   :   8   4   :3530   Median : 74.00  
    ##                                      NA's:1937   5   : 987   Mean   : 74.89  
    ##                                                  NA's:1954   3rd Qu.: 82.00  
    ##                                                              Max.   :224.00  
    ##                                                              NA's   :1946    
    ##  BPXPULS      BPXPTY         BPXML1          BPXSY1          BPXDI1      
    ##  1   :8760   1   :7365   Min.   :110.0   Min.   : 74.0   Min.   :  0.00  
    ##  2   : 171   2   :   5   1st Qu.:130.0   1st Qu.:108.0   1st Qu.: 58.00  
    ##  NA's: 376   NA's:1937   Median :140.0   Median :118.0   Median : 68.00  
    ##                          Mean   :146.2   Mean   :121.2   Mean   : 66.96  
    ##                          3rd Qu.:160.0   3rd Qu.:130.0   3rd Qu.: 76.00  
    ##                          Max.   :240.0   Max.   :230.0   Max.   :116.00  
    ##                          NA's   :1955    NA's   :2160    NA's   :2160    
    ##   BPAEN1         BPXSY2          BPXDI2        BPAEN2         BPXSY3     
    ##  2   :7354   Min.   : 76.0   Min.   :  0.00   1   :  72   Min.   : 74.0  
    ##  NA's:1953   1st Qu.:108.0   1st Qu.: 59.50   2   :7282   1st Qu.:106.0  
    ##              Median :116.0   Median : 68.00   NA's:1953   Median :116.0  
    ##              Mean   :120.1   Mean   : 66.96               Mean   :119.1  
    ##              3rd Qu.:130.0   3rd Qu.: 76.00               3rd Qu.:128.0  
    ##              Max.   :220.0   Max.   :122.00               Max.   :222.0  
    ##              NA's   :2431    NA's   :2431                 NA's   :2548   
    ##      BPXDI3        BPAEN3        LBXWBCSI         LBXLYPCT       LBXMOPCT     
    ##  Min.   :  0.00   1   :  64   Min.   : 1.500   Min.   : 4.9   Min.   : 0.700  
    ##  1st Qu.: 58.00   2   :7290   1st Qu.: 5.800   1st Qu.:26.4   1st Qu.: 6.400  
    ##  Median : 68.00   NA's:1953   Median : 7.000   Median :32.5   Median : 7.800  
    ##  Mean   : 66.72               Mean   : 7.311   Mean   :33.7   Mean   : 7.999  
    ##  3rd Qu.: 76.00               3rd Qu.: 8.475   3rd Qu.:39.8   3rd Qu.: 9.300  
    ##  Max.   :120.00               Max.   :83.200   Max.   :85.5   Max.   :44.500  
    ##  NA's   :2548                 NA's   :1041     NA's   :1057   NA's   :1057    
    ##     LBXNEPCT        LBXEOPCT         LBXBAPCT          LBDLYMNO     
    ##  Min.   : 5.30   Min.   : 0.000   Min.   : 0.0000   Min.   : 0.400  
    ##  1st Qu.:47.60   1st Qu.: 1.600   1st Qu.: 0.4000   1st Qu.: 1.800  
    ##  Median :55.70   Median : 2.400   Median : 0.6000   Median : 2.200  
    ##  Mean   :54.59   Mean   : 3.063   Mean   : 0.6878   Mean   : 2.419  
    ##  3rd Qu.:62.70   3rd Qu.: 3.800   3rd Qu.: 0.8000   3rd Qu.: 2.800  
    ##  Max.   :92.10   Max.   :28.500   Max.   :15.9000   Max.   :71.100  
    ##  NA's   :1057    NA's   :1057     NA's   :1057      NA's   :1058    
    ##     LBDMONO          LBDNENO          LBDEONO          LBDBANO      
    ##  Min.   :0.1000   Min.   : 0.200   Min.   :0.0000   Min.   :0.0000  
    ##  1st Qu.:0.4000   1st Qu.: 2.800   1st Qu.:0.1000   1st Qu.:0.0000  
    ##  Median :0.5000   Median : 3.800   Median :0.2000   Median :0.0000  
    ##  Mean   :0.5668   Mean   : 4.059   Mean   :0.2197   Mean   :0.0414  
    ##  3rd Qu.:0.7000   3rd Qu.: 4.900   3rd Qu.:0.3000   3rd Qu.:0.1000  
    ##  Max.   :5.5000   Max.   :20.500   Max.   :2.9000   Max.   :2.1000  
    ##  NA's   :1058     NA's   :1058     NA's   :1058     NA's   :1058    
    ##     LBXRBCSI         LBXHGB         LBXHCT         LBXMCVSI     
    ##  Min.   :2.490   Min.   : 7.5   Min.   :22.40   Min.   : 55.90  
    ##  1st Qu.:4.360   1st Qu.:12.9   1st Qu.:37.30   1st Qu.: 83.10  
    ##  Median :4.650   Median :13.8   Median :40.20   Median : 86.90  
    ##  Mean   :4.672   Mean   :13.9   Mean   :40.35   Mean   : 86.55  
    ##  3rd Qu.:4.990   3rd Qu.:15.0   3rd Qu.:43.50   3rd Qu.: 90.60  
    ##  Max.   :6.970   Max.   :19.7   Max.   :56.30   Max.   :125.30  
    ##  NA's   :1040    NA's   :1040   NA's   :1040    NA's   :1040    
    ##     LBXMCHSI         LBXMC           LBXRDW        LBXPLTSI       LBXMPSI      
    ##  Min.   :18.00   Min.   :30.00   Min.   : 6.3   Min.   :  28   Min.   : 5.100  
    ##  1st Qu.:28.60   1st Qu.:33.80   1st Qu.:12.1   1st Qu.: 232   1st Qu.: 7.100  
    ##  Median :30.00   Median :34.50   Median :12.5   Median : 273   Median : 7.600  
    ##  Mean   :29.82   Mean   :34.43   Mean   :12.8   Mean   : 283   Mean   : 7.641  
    ##  3rd Qu.:31.30   3rd Qu.:35.10   3rd Qu.:13.1   3rd Qu.: 324   3rd Qu.: 8.100  
    ##  Max.   :60.80   Max.   :42.40   Max.   :37.8   Max.   :1000   Max.   :12.600  
    ##  NA's   :1040    NA's   :1040    NA's   :1040   NA's   :1040   NA's   :1040

We are going to omit all rows which have the response variable missing
(HSD010) - this cuts down on my patient cohort in the study but the
number is still sufficient to be inoformative in the classification task
of the health status of the patient.

**Ommitting the missing patient answer response:**

Exploration of rows where the response variable is NA and subsequent
exclusion from the final cohort:

    completeFun <- function(data, desiredCols){
      completeVec<- complete.cases(data[, desiredCols])
      return(data[completeVec, ])
    }

    full.df<- completeFun(full.df, "HSD010")
    summary(full.df)

    ##       SEQN           HSD010      PEASCST1    PEASCTM1      BPQ150A    
    ##  Min.   :41475   Min.   :1.000   1:6240   Min.   :  45.0   1   :1423  
    ##  1st Qu.:44035   1st Qu.:2.000   2:  11   1st Qu.: 521.0   2   :4828  
    ##  Median :46551   Median :3.000   3: 105   Median : 578.0   NA's: 105  
    ##  Mean   :46548   Mean   :2.779            Mean   : 601.6              
    ##  3rd Qu.:49094   3rd Qu.:3.000            3rd Qu.: 663.0              
    ##  Max.   :51623   Max.   :9.000            Max.   :1521.0              
    ##                                           NA's   :110                 
    ##  BPQ150B     BPQ150C     BPQ150D      BPAARM      BPACSZ         BPXPLS      
    ##  1   :   3   1   :  45   1   : 127   1   :6213   2   :  94   Min.   : 40.00  
    ##  2   :6248   2   :6206   2   :6124   2   :  33   3   :1898   1st Qu.: 64.00  
    ##  NA's: 105   NA's: 105   NA's: 105   8   :   5   4   :3312   Median : 72.00  
    ##                                      NA's: 105   5   : 937   Mean   : 73.78  
    ##                                                  NA's: 115   3rd Qu.: 82.00  
    ##                                                              Max.   :224.00  
    ##                                                              NA's   :109     
    ##  BPXPULS      BPXPTY         BPXML1          BPXSY1          BPXDI1      
    ##  1   :6092   1   :6247   Min.   :110.0   Min.   : 78.0   Min.   :  0.00  
    ##  2   : 159   2   :   4   1st Qu.:140.0   1st Qu.:110.0   1st Qu.: 60.00  
    ##  NA's: 105   NA's: 105   Median :140.0   Median :120.0   Median : 70.00  
    ##                          Mean   :148.5   Mean   :123.3   Mean   : 68.55  
    ##                          3rd Qu.:160.0   3rd Qu.:132.0   3rd Qu.: 78.00  
    ##                          Max.   :240.0   Max.   :230.0   Max.   :116.00  
    ##                          NA's   :115     NA's   :283     NA's   :283     
    ##   BPAEN1         BPXSY2          BPXDI2        BPAEN2         BPXSY3     
    ##  2   :6242   Min.   : 76.0   Min.   :  0.00   1   :  62   Min.   : 74.0  
    ##  NA's: 114   1st Qu.:110.0   1st Qu.: 60.00   2   :6180   1st Qu.:108.0  
    ##              Median :118.0   Median : 70.00   NA's: 114   Median :118.0  
    ##              Mean   :121.9   Mean   : 68.41               Mean   :120.8  
    ##              3rd Qu.:130.0   3rd Qu.: 76.00               3rd Qu.:130.0  
    ##              Max.   :214.0   Max.   :122.00               Max.   :210.0  
    ##              NA's   :432     NA's   :432                  NA's   :510    
    ##      BPXDI3        BPAEN3        LBXWBCSI         LBXLYPCT        LBXMOPCT     
    ##  Min.   :  0.00   1   :  56   Min.   : 1.500   Min.   : 4.90   Min.   : 0.700  
    ##  1st Qu.: 60.00   2   :6186   1st Qu.: 5.700   1st Qu.:25.20   1st Qu.: 6.300  
    ##  Median : 68.00   NA's: 114   Median : 6.900   Median :30.60   Median : 7.700  
    ##  Mean   : 68.13               Mean   : 7.266   Mean   :31.04   Mean   : 7.929  
    ##  3rd Qu.: 76.00               3rd Qu.: 8.400   3rd Qu.:36.40   3rd Qu.: 9.200  
    ##  Max.   :120.00               Max.   :83.200   Max.   :85.50   Max.   :44.500  
    ##  NA's   :510                  NA's   :347      NA's   :360     NA's   :360     
    ##     LBXNEPCT        LBXEOPCT         LBXBAPCT         LBDLYMNO     
    ##  Min.   : 6.50   Min.   : 0.100   Min.   :0.0000   Min.   : 0.400  
    ##  1st Qu.:51.30   1st Qu.: 1.500   1st Qu.:0.4000   1st Qu.: 1.700  
    ##  Median :57.90   Median : 2.400   Median :0.6000   Median : 2.100  
    ##  Mean   :57.44   Mean   : 2.963   Mean   :0.6778   Mean   : 2.208  
    ##  3rd Qu.:64.10   3rd Qu.: 3.700   3rd Qu.:0.9000   3rd Qu.: 2.600  
    ##  Max.   :89.90   Max.   :28.500   Max.   :7.3000   Max.   :71.100  
    ##  NA's   :360     NA's   :360      NA's   :360      NA's   :361     
    ##     LBDMONO          LBDNENO          LBDEONO          LBDBANO      
    ##  Min.   :0.1000   Min.   : 0.200   Min.   :0.0000   Min.   :0.0000  
    ##  1st Qu.:0.4000   1st Qu.: 3.100   1st Qu.:0.1000   1st Qu.:0.0000  
    ##  Median :0.5000   Median : 4.000   Median :0.2000   Median :0.0000  
    ##  Mean   :0.5572   Mean   : 4.242   Mean   :0.2109   Mean   :0.0398  
    ##  3rd Qu.:0.7000   3rd Qu.: 5.100   3rd Qu.:0.3000   3rd Qu.:0.1000  
    ##  Max.   :5.5000   Max.   :20.500   Max.   :2.9000   Max.   :2.1000  
    ##  NA's   :361      NA's   :361      NA's   :361      NA's   :361     
    ##     LBXRBCSI         LBXHGB          LBXHCT         LBXMCVSI     
    ##  Min.   :2.560   Min.   : 7.50   Min.   :24.50   Min.   : 58.70  
    ##  1st Qu.:4.360   1st Qu.:13.10   1st Qu.:38.40   1st Qu.: 85.30  
    ##  Median :4.690   Median :14.20   Median :41.30   Median : 88.50  
    ##  Mean   :4.701   Mean   :14.21   Mean   :41.29   Mean   : 88.03  
    ##  3rd Qu.:5.050   3rd Qu.:15.40   3rd Qu.:44.40   3rd Qu.: 91.50  
    ##  Max.   :6.970   Max.   :19.70   Max.   :56.30   Max.   :125.30  
    ##  NA's   :346     NA's   :346     NA's   :346     NA's   :346     
    ##     LBXMCHSI         LBXMC           LBXRDW         LBXPLTSI     
    ##  Min.   :18.40   Min.   :30.30   Min.   :10.80   Min.   :  28.0  
    ##  1st Qu.:29.20   1st Qu.:33.80   1st Qu.:12.20   1st Qu.: 224.0  
    ##  Median :30.50   Median :34.40   Median :12.60   Median : 261.0  
    ##  Mean   :30.29   Mean   :34.39   Mean   :12.87   Mean   : 268.7  
    ##  3rd Qu.:31.70   3rd Qu.:35.00   3rd Qu.:13.20   3rd Qu.: 305.0  
    ##  Max.   :60.80   Max.   :39.00   Max.   :37.80   Max.   :1000.0  
    ##  NA's   :346     NA's   :346     NA's   :346     NA's   :346     
    ##     LBXMPSI      
    ##  Min.   : 5.100  
    ##  1st Qu.: 7.200  
    ##  Median : 7.700  
    ##  Mean   : 7.769  
    ##  3rd Qu.: 8.200  
    ##  Max.   :12.600  
    ##  NA's   :346

**Additional step of feature selection**:

    # indexing the rows where all columns have missing values but omitting the sequence id for the patient, as well as the repsonse variable, and blood pressure variable - none are missing there by our previous clean up and definition.
    ind <- apply(full.df[,-c(1,2,3)], 1, function(x) all(is.na(x)))

    cat(sum(ind), "is the number of rows where all values are missing and therefore should be discarded\n")

    ## 20 is the number of rows where all values are missing and therefore should be discarded

    full.df <-full.df[ !ind, ]

    cat("New dataset dimensions are the following:", dim(full.df)[1], "rows,", dim(full.df)[2], "features")

    ## New dataset dimensions are the following: 6336 rows, 43 features

### Recoding of the Response HSD010 variable:

The response variable of the health quality measured on the following
scale:

![Description of the response variable scale](response_var.png)

Based on this description, I am using the first three entries to encode
1 as in “good health” and the rest as “not good/ difficulty answering”,
which would indicate additional research needed into that area/ answer.

    # original breakdown of the categories:
    table(full.df$HSD010)

    ## 
    ##    1    2    3    4    5    9 
    ##  669 1735 2511 1176  243    2

    full.df$HSD010_re<- ifelse(full.df$HSD010 == 1|full.df$HSD010 == 2|full.df$HSD010 == 3, 1, 0)

    cat("new breakdown of the response",table(full.df$HSD010_re))

    ## new breakdown of the response 1421 4915

    full.df$HSD010_re<- as.factor(full.df$HSD010_re)

### Distribution exploration:

This is a distribution plot of all the numeric features - it is hard to
analyse anything with such density of plotting, so I will break it down
to parts.

    full.df  %>%  dplyr::select(-SEQN, -HSD010) %>% 
      dplyr::select_if(is.numeric) %>%  gather(cols, value) %>% 
      ggplot(aes(x = value)) +
      geom_histogram() + 
      facet_wrap(.~cols, scale = "free") +
      ggtitle("Distributions of the numeric\n values in the full blood dataset- \n  labs and pressure") + 
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  + 
      theme(plot.title = element_text(hjust = 0.5))

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 9270 rows containing non-finite values (stat_bin).

![](DA5030.Proj.Stoma_files/figure-markdown_strict/distr%20fulldf%20all-1.png)

**Plane 1:**

Plotting the disrtibution of the kept features: BPXDI1,2,3 visble have
outliers on the left tail - just about zero. Since this values represent
Diastolic pressure and inlikely to realistically be 0, these should be
imputed/ omitted as these are not real values in terms of
meaningfulness. Additionally, the distribution of some of the features
are skewed(BPXSY1) and will undergo transformation.

    full.df  %>%  dplyr::select(BPXDI1, BPXDI2,BPXDI3, BPXPLS, BPXSY1, BPXSY2, BPXSY3) %>% 
      dplyr::select_if(is.numeric) %>%  
      gather(cols, value) %>%  
      ggplot(aes(x = value)) + 
      geom_histogram() + facet_wrap(.~cols, scale = "free") + 
      ggtitle("Distributions of the numeric\n values in the full blood dataset- \n  labs and pressure") + 
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  + 
      theme(plot.title = element_text(hjust = 0.5))

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 2419 rows containing non-finite values (stat_bin).

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-7-1.png)
Evident outliers and skewness in the top three graphs - transphormation
and imputation to be performed.

**Plane 2:**

    full.df  %>%  
      dplyr::select(PEASCTM1, BPXML1, LBXWBCSI, LBXLYPCT, 
                    LBXMOPCT, LBXNEPCT, LBXEOPCT,LBXBAPCT, LBDLYMNO ) %>% 
      dplyr::select_if(is.numeric) %>%  
      gather(cols, value) %>% 
      ggplot(aes(x = value)) + 
      geom_histogram() + 
      facet_wrap(.~cols, scale = "free") + 
      ggtitle("Distributions of the numeric\n values in the full blood dataset- \n  labs and pressure - PART2") + 
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  +
      theme(plot.title = element_text(hjust = 0.5))

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 2553 rows containing non-finite values (stat_bin).

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-8-1.png)

Additionally, PEASCTM1 - Blood Pressure Time in Seconds also has
outliers on the left - we do no expect this time to be 0 realistically,
therefore, imputation of the outliers with median values will take
place. Additional skewness of data in LBXEOPCT and PEASCTM1.

In this case, skewness and “non scaleness” and “non centerness” will be
issues in my imputation procedure if I choose to proceed with kNN.

**Plane 3:**

    full.df  %>%  
      dplyr::select(LBDMONO, LBDNENO, LBDEONO, LBDBANO, 
                    LBXRBCSI, LBXHGB, LBXHCT,LBXMCVSI,
                    LBXMCHSI, LBXMC, LBXRDW, LBXPLTSI, LBXMPSI ) %>%
      dplyr::select_if(is.numeric) %>% 
      gather(cols, value) %>%  
      ggplot(aes(x = value)) + 
      geom_histogram() + 
      facet_wrap(.~cols, scale = "free") + 
      ggtitle("Distributions of the numeric\n values in the full blood dataset- \n  labs and pressure - PART3") + 
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  + 
      theme(plot.title = element_text(hjust = 0.5)) + 
      theme(axis.text.x = element_text(size = 4))

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 4298 rows containing non-finite values (stat_bin).

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-9-1.png)

### Center and Scale numerical

    # scale and center numerical variables:
    full.df<- full.df %>%
       mutate_at(c("PEASCTM1", "BPXPLS", "BPXML1",
                   "BPXSY1", "BPXDI1", "BPXDI2",
                   "BPXDI3", "BPXSY2", "BPXSY3", 
                   "LBXWBCSI", "LBXLYPCT", "LBXMOPCT",
                   "LBXNEPCT", "LBXEOPCT", "LBXBAPCT",
                   "LBDLYMNO", "LBDMONO", "LBDNENO", "LBDEONO", 
                   "LBDBANO", "LBXRBCSI", "LBXHGB", "LBXHCT",
                   "LBXMCVSI", "LBXMCHSI", "LBXMC", "LBXRDW",
                   "LBXPLTSI", "LBXMPSI"), 
                 funs(c(scale(., center = TRUE, scale = TRUE))))

    ## Warning: `funs()` is deprecated as of dplyr 0.8.0.
    ## Please use a list of either functions or lambdas: 
    ## 
    ##   # Simple named list: 
    ##   list(mean = mean, median = median)
    ## 
    ##   # Auto named with `tibble::lst()`: 
    ##   tibble::lst(mean, median)
    ## 
    ##   # Using lambdas
    ##   list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))
    ## This warning is displayed once every 8 hours.
    ## Call `lifecycle::last_warnings()` to see where this warning was generated.

    summary(full.df)

    ##       SEQN           HSD010      PEASCST1    PEASCTM1       BPQ150A    
    ##  Min.   :41475   Min.   :1.000   1:6240   Min.   :-4.5952   1   :1423  
    ##  1st Qu.:44033   1st Qu.:2.000   2:  11   1st Qu.:-0.6653   2   :4828  
    ##  Median :46548   Median :3.000   3:  85   Median :-0.1946   NA's:  85  
    ##  Mean   :46546   Mean   :2.779            Mean   : 0.0000              
    ##  3rd Qu.:49094   3rd Qu.:3.000            3rd Qu.: 0.5071              
    ##  Max.   :51623   Max.   :9.000            Max.   : 7.5908              
    ##                                           NA's   :90                   
    ##  BPQ150B     BPQ150C     BPQ150D      BPAARM      BPACSZ         BPXPLS       
    ##  1   :   3   1   :  45   1   : 127   1   :6213   2   :  94   Min.   :-2.6579  
    ##  2   :6248   2   :6206   2   :6124   2   :  33   3   :1898   1st Qu.:-0.7695  
    ##  NA's:  85   NA's:  85   NA's:  85   8   :   5   4   :3312   Median :-0.1401  
    ##                                      NA's:  85   5   : 937   Mean   : 0.0000  
    ##                                                  NA's:  95   3rd Qu.: 0.6467  
    ##                                                              Max.   :11.8197  
    ##                                                              NA's   :89       
    ##  BPXPULS      BPXPTY         BPXML1            BPXSY1            BPXDI1       
    ##  1   :6092   1   :6247   Min.   :-2.0306   Min.   :-2.3689   Min.   :-5.0549  
    ##  2   : 159   2   :   4   1st Qu.:-0.4492   1st Qu.:-0.6948   1st Qu.:-0.6304  
    ##  NA's:  85   NA's:  85   Median :-0.4492   Median :-0.1717   Median : 0.1070  
    ##                          Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
    ##                          3rd Qu.: 0.6051   3rd Qu.: 0.4561   3rd Qu.: 0.6969  
    ##                          Max.   : 4.8222   Max.   : 5.5830   Max.   : 3.4991  
    ##                          NA's   :95        NA's   :263       NA's   :263      
    ##   BPAEN1         BPXSY2            BPXDI2         BPAEN2         BPXSY3       
    ##  2   :6242   Min.   :-2.4925   Min.   :-5.1043   1   :  62   Min.   :-2.6008  
    ##  NA's:  94   1st Qu.:-0.6452   1st Qu.:-0.6272   2   :6180   1st Qu.:-0.7098  
    ##              Median :-0.2106   Median : 0.1190   NA's:  94   Median :-0.1536  
    ##              Mean   : 0.0000   Mean   : 0.0000               Mean   : 0.0000  
    ##              3rd Qu.: 0.4414   3rd Qu.: 0.5667               3rd Qu.: 0.5138  
    ##              Max.   : 5.0053   Max.   : 3.9992               Max.   : 4.9632  
    ##              NA's   :412       NA's   :412                   NA's   :490      
    ##      BPXDI3         BPAEN3        LBXWBCSI          LBXLYPCT      
    ##  Min.   :-5.0586   1   :  56   Min.   :-2.3709   Min.   :-2.9910  
    ##  1st Qu.:-0.6033   2   :6186   1st Qu.:-0.6439   1st Qu.:-0.6679  
    ##  Median :-0.0093   NA's:  94   Median :-0.1505   Median :-0.0500  
    ##  Mean   : 0.0000               Mean   : 0.0000   Mean   : 0.0000  
    ##  3rd Qu.: 0.5847               3rd Qu.: 0.4663   3rd Qu.: 0.6138  
    ##  Max.   : 3.8519               Max.   :31.2230   Max.   : 6.2326  
    ##  NA's   :490                   NA's   :327       NA's   :340      
    ##     LBXMOPCT          LBXNEPCT          LBXEOPCT          LBXBAPCT      
    ##  Min.   :-3.0244   Min.   :-5.1562   Min.   :-1.2632   Min.   :-1.4503  
    ##  1st Qu.:-0.6814   1st Qu.:-0.6211   1st Qu.:-0.6455   1st Qu.:-0.5944  
    ##  Median :-0.0956   Median : 0.0471   Median :-0.2484   Median :-0.1664  
    ##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
    ##  3rd Qu.: 0.5319   3rd Qu.: 0.6747   3rd Qu.: 0.3252   3rd Qu.: 0.4756  
    ##  Max.   :15.3012   Max.   : 3.2865   Max.   :11.2678   Max.   :14.1707  
    ##  NA's   :340       NA's   :340       NA's   :340       NA's   :340      
    ##     LBDLYMNO          LBDMONO           LBDNENO           LBDEONO       
    ##  Min.   :-1.3814   Min.   :-2.2594   Min.   :-2.3899   Min.   :-1.1932  
    ##  1st Qu.:-0.3880   1st Qu.:-0.7768   1st Qu.:-0.6752   1st Qu.:-0.6276  
    ##  Median :-0.0823   Median :-0.2827   Median :-0.1430   Median :-0.0619  
    ##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
    ##  3rd Qu.: 0.2999   3rd Qu.: 0.7057   3rd Qu.: 0.5073   3rd Qu.: 0.5038  
    ##  Max.   :52.6493   Max.   :24.4265   Max.   : 9.6128   Max.   :15.2112  
    ##  NA's   :341       NA's   :341       NA's   :341       NA's   :341      
    ##     LBDBANO           LBXRBCSI           LBXHGB            LBXHCT       
    ##  Min.   :-0.6318   Min.   :-4.3044   Min.   :-4.2952   Min.   :-3.9572  
    ##  1st Qu.:-0.6318   1st Qu.:-0.6861   1st Qu.:-0.7097   1st Qu.:-0.6804  
    ##  Median :-0.6318   Median :-0.0228   Median :-0.0054   Median : 0.0033  
    ##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
    ##  3rd Qu.: 0.9550   3rd Qu.: 0.7009   3rd Qu.: 0.7629   3rd Qu.: 0.7340  
    ##  Max.   :32.6906   Max.   : 4.5604   Max.   : 3.5161   Max.   : 3.5393  
    ##  NA's   :341       NA's   :326       NA's   :326       NA's   :326      
    ##     LBXMCVSI          LBXMCHSI           LBXMC             LBXRDW       
    ##  Min.   :-5.1848   Min.   :-5.0528   Min.   :-4.2009   Min.   :-1.7016  
    ##  1st Qu.:-0.4833   1st Qu.:-0.4646   1st Qu.:-0.6052   1st Qu.:-0.5482  
    ##  Median : 0.0822   Median : 0.0877   Median : 0.0112   Median :-0.2187  
    ##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
    ##  3rd Qu.: 0.6125   3rd Qu.: 0.5975   3rd Qu.: 0.6276   3rd Qu.: 0.2756  
    ##  Max.   : 6.5865   Max.   :12.9601   Max.   : 4.7369   Max.   :20.5415  
    ##  NA's   :326       NA's   :326       NA's   :326       NA's   :326      
    ##     LBXPLTSI          LBXMPSI        HSD010_re
    ##  Min.   :-3.4594   Min.   :-3.1918   0:1421   
    ##  1st Qu.:-0.6425   1st Qu.:-0.6808   1:4915   
    ##  Median :-0.1107   Median :-0.0830            
    ##  Mean   : 0.0000   Mean   : 0.0000            
    ##  3rd Qu.: 0.5216   3rd Qu.: 0.5149            
    ##  Max.   :10.5102   Max.   : 5.7759            
    ##  NA's   :326       NA's   :326

Upon scaling and centering the dataset ( as evident from the summary
where the mean for all of my numerical features is 0), we can proceed to
remove the outliers from some of the numerical features and impute
numerical missing values.

Outlier exploration:
--------------------

This dataset contains scaled and centered numerica values. Next, I am
going to explore what to do with the outliers in the dataset. As a
result of scaling and centering, some of the skeness issues became less
significant. Other feature would still need to be transformed -

“BPXDI3”, “BPXSY2”, “BPXSY3”, “LBXWBCSI”, “LBXLYPCT”, “LBXMOPCT”,
“LBXNEPCT”, “LBXEOPCT”, “LBXBAPCT”, “LBDLYMNO”, “LBDMONO”, “LBDNENO”,
“LBDEONO”, “LBDBANO”, “LBXRBCSI”, “LBXHGB”, “LBXHCT”, “LBXMCVSI”,
“LBXMCHSI”, “LBXMC”, “LBXRDW”, “LBXPLTSI”, “LBXMPSI”

    full.df %>%
      dplyr::select(c("PEASCTM1", "BPXPLS", "BPXML1", 
                      "BPXSY1", "BPXDI1", "BPXDI2")) %>% 
      gather(cols, value) %>%  
      ggplot(aes(x = value)) + 
      geom_histogram() + 
      facet_wrap(.~cols, scale = "free") + 
      ggtitle("Distributions of the numeric\n values in the full blood dataset- \n  labs and pressure") + 
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  + 
      theme(plot.title = element_text(hjust = 0.5))

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 1212 rows containing non-finite values (stat_bin).

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-11-1.png)

Part 2:

    full.df %>% 
      dplyr::select(c("BPXDI3", "BPXSY2", "BPXSY3", 
                      "LBXWBCSI", "LBXLYPCT", "LBXMOPCT",
                      "LBXNEPCT", "LBXEOPCT", "LBXBAPCT", 
                      "LBDLYMNO", "LBDMONO")) %>% 
      gather(cols, value) %>%  
      ggplot(aes(x = value)) + geom_histogram() + 
      facet_wrap(.~cols, scale = "free") + 
      ggtitle("Distributions of the numeric\n values in the full blood dataset- \n  labs and pressure") + 
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  + 
      theme(plot.title = element_text(hjust = 0.5))

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 4101 rows containing non-finite values (stat_bin).

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-12-1.png)

Part 3:

    full.df %>% 
      dplyr::select(c("LBDNENO", "LBDEONO", "LBDBANO",
                      "LBXRBCSI", "LBXHGB", "LBXHCT", "LBXMCVSI", 
                      "LBXMCHSI", "LBXMC", "LBXRDW", "LBXPLTSI", 
                      "LBXMPSI")) %>% gather(cols, value) %>% 
      ggplot(aes(x = value)) +
      geom_histogram() + 
      facet_wrap(.~cols, scale = "free") + 
      ggtitle("Distributions of the numeric\n values in the full blood dataset- \n  labs and pressure") + 
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  + 
      theme(plot.title = element_text(hjust = 0.5))

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 3957 rows containing non-finite values (stat_bin).

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-13-1.png)

Outlier removal:
----------------

As mentioned above, three numerical features contain outliers (in the
non transformed values, Diastolic blood pressure of ~0). These should be
substituted - I will use median imputation for these values - BPXDI1,
BPXDI2, BPXDI3 with the cutoff of 3.5 z score.

    full.df$BPXDI1[abs(full.df$BPXDI1) > 3.5]<- median(full.df$BPXDI1, na.rm = TRUE)
    full.df$BPXDI2[abs(full.df$BPXDI2) > 3.5]<- median(full.df$BPXDI2, na.rm = TRUE)
    full.df$BPXDI3[abs(full.df$BPXDI3) > 3.5]<- median(full.df$BPXDI3, na.rm = TRUE)
    full.df$PEASCTM1[abs(full.df$PEASCTM1) >3.5]<- median(full.df$PEASCTM1, na.rm = TRUE)

**Checking the distribution after scaling and removing the outliers**:

    full.df  %>%  
      dplyr::select(BPXDI1, BPXDI2, BPXDI3, PEASCTM1) %>% 
      dplyr::select_if(is.numeric) %>%  gather(cols, value) %>%  
      ggplot(aes(x = value)) + 
      geom_histogram() + facet_grid(.~cols, scale = "free") + 
      ggtitle("Distributions of the scaled, mediam imputed blood exam values") +
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  + 
      theme(plot.title = element_text(hjust = 0.5))

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 1255 rows containing non-finite values (stat_bin).

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-15-1.png)

This new dataset contains no outliers in the numerical features of the
Diastolic Blood Pressure. However, the distribution of PEASCTM1 is still
somewhat trimodal - I am going to proceed with this data.

At this point the dataset still contains many missing values in some of
the feaures - I am going to proceed with imputation.

    full.df %>%
      summary() 

    ##       SEQN           HSD010      PEASCST1    PEASCTM1        BPQ150A    
    ##  Min.   :41475   Min.   :1.000   1:6240   Min.   :-3.45581   1   :1423  
    ##  1st Qu.:44033   1st Qu.:2.000   2:  11   1st Qu.:-0.65699   2   :4828  
    ##  Median :46548   Median :3.000   3:  85   Median :-0.19465   NA's:  85  
    ##  Mean   :46546   Mean   :2.779            Mean   :-0.02416              
    ##  3rd Qu.:49094   3rd Qu.:3.000            3rd Qu.: 0.48235              
    ##  Max.   :51623   Max.   :9.000            Max.   : 3.47931              
    ##                                           NA's   :90                    
    ##  BPQ150B     BPQ150C     BPQ150D      BPAARM      BPACSZ         BPXPLS       
    ##  1   :   3   1   :  45   1   : 127   1   :6213   2   :  94   Min.   :-2.6579  
    ##  2   :6248   2   :6206   2   :6124   2   :  33   3   :1898   1st Qu.:-0.7695  
    ##  NA's:  85   NA's:  85   NA's:  85   8   :   5   4   :3312   Median :-0.1401  
    ##                                      NA's:  85   5   : 937   Mean   : 0.0000  
    ##                                                  NA's:  95   3rd Qu.: 0.6467  
    ##                                                              Max.   :11.8197  
    ##                                                              NA's   :89       
    ##  BPXPULS      BPXPTY         BPXML1            BPXSY1            BPXDI1        
    ##  1   :6092   1   :6247   Min.   :-2.0306   Min.   :-2.3689   Min.   :-3.43260  
    ##  2   : 159   2   :   4   1st Qu.:-0.4492   1st Qu.:-0.6948   1st Qu.:-0.63044  
    ##  NA's:  85   NA's:  85   Median :-0.4492   Median :-0.1717   Median : 0.10697  
    ##                          Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.03621  
    ##                          3rd Qu.: 0.6051   3rd Qu.: 0.4561   3rd Qu.: 0.69690  
    ##                          Max.   : 4.8222   Max.   : 5.5830   Max.   : 3.49906  
    ##                          NA's   :95        NA's   :263       NA's   :263       
    ##   BPAEN1         BPXSY2            BPXDI2         BPAEN2         BPXSY3       
    ##  2   :6242   Min.   :-2.4925   Min.   :-3.4627   1   :  62   Min.   :-2.6008  
    ##  NA's:  94   1st Qu.:-0.6452   1st Qu.:-0.6272   2   :6180   1st Qu.:-0.7098  
    ##              Median :-0.2106   Median : 0.1190   NA's:  94   Median :-0.1536  
    ##              Mean   : 0.0000   Mean   : 0.0275               Mean   : 0.0000  
    ##              3rd Qu.: 0.4414   3rd Qu.: 0.5667               3rd Qu.: 0.5138  
    ##              Max.   : 5.0053   Max.   : 3.4022               Max.   : 4.9632  
    ##              NA's   :412       NA's   :412                   NA's   :490      
    ##      BPXDI3         BPAEN3        LBXWBCSI          LBXLYPCT      
    ##  Min.   :-3.4250   1   :  56   Min.   :-2.3709   Min.   :-2.9910  
    ##  1st Qu.:-0.6033   2   :6186   1st Qu.:-0.6439   1st Qu.:-0.6679  
    ##  Median :-0.0093   NA's:  94   Median :-0.1505   Median :-0.0500  
    ##  Mean   : 0.0311               Mean   : 0.0000   Mean   : 0.0000  
    ##  3rd Qu.: 0.5847               3rd Qu.: 0.4663   3rd Qu.: 0.6138  
    ##  Max.   : 3.4064               Max.   :31.2230   Max.   : 6.2326  
    ##  NA's   :490                   NA's   :327       NA's   :340      
    ##     LBXMOPCT          LBXNEPCT          LBXEOPCT          LBXBAPCT      
    ##  Min.   :-3.0244   Min.   :-5.1562   Min.   :-1.2632   Min.   :-1.4503  
    ##  1st Qu.:-0.6814   1st Qu.:-0.6211   1st Qu.:-0.6455   1st Qu.:-0.5944  
    ##  Median :-0.0956   Median : 0.0471   Median :-0.2484   Median :-0.1664  
    ##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
    ##  3rd Qu.: 0.5319   3rd Qu.: 0.6747   3rd Qu.: 0.3252   3rd Qu.: 0.4756  
    ##  Max.   :15.3012   Max.   : 3.2865   Max.   :11.2678   Max.   :14.1707  
    ##  NA's   :340       NA's   :340       NA's   :340       NA's   :340      
    ##     LBDLYMNO          LBDMONO           LBDNENO           LBDEONO       
    ##  Min.   :-1.3814   Min.   :-2.2594   Min.   :-2.3899   Min.   :-1.1932  
    ##  1st Qu.:-0.3880   1st Qu.:-0.7768   1st Qu.:-0.6752   1st Qu.:-0.6276  
    ##  Median :-0.0823   Median :-0.2827   Median :-0.1430   Median :-0.0619  
    ##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
    ##  3rd Qu.: 0.2999   3rd Qu.: 0.7057   3rd Qu.: 0.5073   3rd Qu.: 0.5038  
    ##  Max.   :52.6493   Max.   :24.4265   Max.   : 9.6128   Max.   :15.2112  
    ##  NA's   :341       NA's   :341       NA's   :341       NA's   :341      
    ##     LBDBANO           LBXRBCSI           LBXHGB            LBXHCT       
    ##  Min.   :-0.6318   Min.   :-4.3044   Min.   :-4.2952   Min.   :-3.9572  
    ##  1st Qu.:-0.6318   1st Qu.:-0.6861   1st Qu.:-0.7097   1st Qu.:-0.6804  
    ##  Median :-0.6318   Median :-0.0228   Median :-0.0054   Median : 0.0033  
    ##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
    ##  3rd Qu.: 0.9550   3rd Qu.: 0.7009   3rd Qu.: 0.7629   3rd Qu.: 0.7340  
    ##  Max.   :32.6906   Max.   : 4.5604   Max.   : 3.5161   Max.   : 3.5393  
    ##  NA's   :341       NA's   :326       NA's   :326       NA's   :326      
    ##     LBXMCVSI          LBXMCHSI           LBXMC             LBXRDW       
    ##  Min.   :-5.1848   Min.   :-5.0528   Min.   :-4.2009   Min.   :-1.7016  
    ##  1st Qu.:-0.4833   1st Qu.:-0.4646   1st Qu.:-0.6052   1st Qu.:-0.5482  
    ##  Median : 0.0822   Median : 0.0877   Median : 0.0112   Median :-0.2187  
    ##  Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
    ##  3rd Qu.: 0.6125   3rd Qu.: 0.5975   3rd Qu.: 0.6276   3rd Qu.: 0.2756  
    ##  Max.   : 6.5865   Max.   :12.9601   Max.   : 4.7369   Max.   :20.5415  
    ##  NA's   :326       NA's   :326       NA's   :326       NA's   :326      
    ##     LBXPLTSI          LBXMPSI        HSD010_re
    ##  Min.   :-3.4594   Min.   :-3.1918   0:1421   
    ##  1st Qu.:-0.6425   1st Qu.:-0.6808   1:4915   
    ##  Median :-0.1107   Median :-0.0830            
    ##  Mean   : 0.0000   Mean   : 0.0000            
    ##  3rd Qu.: 0.5216   3rd Qu.: 0.5149            
    ##  Max.   :10.5102   Max.   : 5.7759            
    ##  NA's   :326       NA's   :326

Since this dataset has mixed types of data- categorical and numeric -
imputation technqiues can really vary. I will explore two versions - all
numerical features will be imputed using kNN, and all categorical
features will be imputed using median. Alternatively, I found a package
called **mice**, which utilizes a number of decision rules to impute
both categorical and numeric features.

Imputation:
-----------

### kNN imputation of the numeric values:

For imputation of numeric features with knn, there cannot be rows which
have all features missing - I am going to check if there are any rows
like that and exclude them from this version of the dataframe for the
model: Although the kNN imputation does not require a normally
distributed data, we need this assumption for the neural network and
logistic regression, as well as scale and center the data for knn.

    nm.integer<- full.df %>% 
      dplyr::select(-c(SEQN, HSD010, HSD010_re)) %>% 
      dplyr::select_if(is.numeric) %>% names()

    # the list of numerical columns I will knn impute in the next step:
    paste(nm.integer, collapse = "','")

    ## [1] "PEASCTM1','BPXPLS','BPXML1','BPXSY1','BPXDI1','BPXSY2','BPXDI2','BPXSY3','BPXDI3','LBXWBCSI','LBXLYPCT','LBXMOPCT','LBXNEPCT','LBXEOPCT','LBXBAPCT','LBDLYMNO','LBDMONO','LBDNENO','LBDEONO','LBDBANO','LBXRBCSI','LBXHGB','LBXHCT','LBXMCVSI','LBXMCHSI','LBXMC','LBXRDW','LBXPLTSI','LBXMPSI"

Numerical columns list:

‘PEASCTM1’,‘BPXPLS’,‘BPXML1’,‘BPXSY1’,‘BPXDI1’,‘BPXSY2’,‘BPXDI2’,‘BPXSY3’,‘BPXDI3’,‘LBXWBCSI’,‘LBXLYPCT’,‘LBXMOPCT’,‘LBXNEPCT’,
‘LBXEOPCT’,‘LBXBAPCT’,‘LBDLYMNO’,‘LBDMONO’,‘LBDNENO’,‘LBDEONO’,‘LBDBANO’,‘LBXRBCSI’,‘LBXHGB’,‘LBXHCT’,‘LBXMCVSI’,
‘LBXMCHSI’,‘LBXMC’,‘LBXRDW’,‘LBXPLTSI’,‘LBXMPSI’

    # these are the rows that contain missing values in all numerical features and therefore cannot be imputed with knn in caret package. I am going to delete these rows from the dataset:
    badrow<- apply(full.df[, c('PEASCTM1','BPXPLS','BPXML1',
                               'BPXSY1','BPXDI1','BPXSY2','BPXDI2',
                               'BPXSY3','BPXDI3','LBXWBCSI','LBXLYPCT',
                               'LBXMOPCT','LBXNEPCT','LBXEOPCT','LBXBAPCT',
                               'LBDLYMNO','LBDMONO','LBDNENO','LBDEONO',
                               'LBDBANO','LBXRBCSI','LBXHGB','LBXHCT',
                               'LBXMCVSI','LBXMCHSI','LBXMC','LBXRDW',
                               'LBXPLTSI','LBXMPSI')], 1, function(x) all(is.na(x)))


    cat(sum(badrow), "is a reasonable number of rows to exclude from the dataset to achieve better imputation with knn.")

    ## 2 is a reasonable number of rows to exclude from the dataset to achieve better imputation with knn.

    # preprocessign with knn 
    full_imputed_knn<- preProcess(full.df[!badrow, c('PEASCTM1','BPXPLS','BPXML1','BPXSY1','BPXDI1',
                                                     'BPXSY2','BPXDI2','BPXSY3','BPXDI3','LBXWBCSI',
                                                     'LBXLYPCT','LBXMOPCT','LBXNEPCT','LBXEOPCT',
                                                     'LBXBAPCT','LBDLYMNO','LBDMONO','LBDNENO','LBDEONO',
                                                     'LBDBANO','LBXRBCSI','LBXHGB','LBXHCT',
                                                     'LBXMCVSI','LBXMCHSI','LBXMC','LBXRDW',
                                                     'LBXPLTSI','LBXMPSI')], 
                                  method = "knnImpute", k = 6)

    # fully imputed dataframe:
    full_imputed_df <- predict(full_imputed_knn,full.df[!badrow, 
                                                        c('PEASCTM1','BPXPLS','BPXML1',
                                                                   'BPXSY1','BPXDI1','BPXSY2',
                                                                   'BPXDI2','BPXSY3','BPXDI3','LBXWBCSI',
                                                                   'LBXLYPCT', 'LBXMOPCT','LBXNEPCT','LBXEOPCT','LBXBAPCT',
                                                                   'LBDLYMNO','LBDMONO','LBDNENO','LBDEONO',
                                                                   'LBDBANO','LBXRBCSI','LBXHGB',
                                                                   'LBXHCT','LBXMCVSI','LBXMCHSI',
                                                                   'LBXMC','LBXRDW','LBXPLTSI','LBXMPSI')])

    # no missing values in th enumeric featuers. 
    summary(full_imputed_df)

    ##     PEASCTM1             BPXPLS              BPXML1         
    ##  Min.   :-3.745598   Min.   :-2.657940   Min.   :-2.030578  
    ##  1st Qu.:-0.681719   1st Qu.:-0.769549   1st Qu.:-0.449175  
    ##  Median :-0.186091   Median :-0.140085   Median :-0.449175  
    ##  Mean   : 0.000279   Mean   :-0.000769   Mean   :-0.000617  
    ##  3rd Qu.: 0.543833   3rd Qu.: 0.646744   3rd Qu.: 0.605094  
    ##  Max.   : 3.823985   Max.   :11.819724   Max.   : 4.822169  
    ##      BPXSY1              BPXDI1              BPXSY2             BPXDI2         
    ##  Min.   :-2.368885   Min.   :-3.821046   Min.   :-2.49253   Min.   :-3.818122  
    ##  1st Qu.:-0.694813   1st Qu.:-0.571881   1st Qu.:-0.64524   1st Qu.:-0.661804  
    ##  Median :-0.171666   Median : 0.077952   Median :-0.21058   Median : 0.072856  
    ##  Mean   : 0.009332   Mean   : 0.007978   Mean   :-0.00551   Mean   :-0.004383  
    ##  3rd Qu.: 0.486628   3rd Qu.: 0.619480   3rd Qu.: 0.44141   3rd Qu.: 0.589839  
    ##  Max.   : 5.582955   Max.   : 3.814492   Max.   : 5.00530   Max.   : 3.691738  
    ##      BPXSY3              BPXDI3             LBXWBCSI        
    ##  Min.   :-2.600810   Min.   :-3.789903   Min.   :-2.370928  
    ##  1st Qu.:-0.709806   1st Qu.:-0.695703   1st Qu.:-0.602827  
    ##  Median :-0.153629   Median :-0.044293   Median :-0.109403  
    ##  Mean   :-0.008757   Mean   :-0.007668   Mean   : 0.002487  
    ##  3rd Qu.: 0.402548   3rd Qu.: 0.607118   3rd Qu.: 0.445698  
    ##  Max.   : 4.963203   Max.   : 3.701318   Max.   :31.222997  
    ##     LBXLYPCT            LBXMOPCT            LBXNEPCT            LBXEOPCT       
    ##  Min.   :-2.990976   Min.   :-3.024391   Min.   :-5.156229   Min.   :-1.26320  
    ##  1st Qu.:-0.645028   1st Qu.:-0.639555   1st Qu.:-0.590691   1st Qu.:-0.60135  
    ##  Median :-0.049958   Median :-0.095645   Median : 0.052128   Median :-0.24837  
    ##  Mean   :-0.001843   Mean   :-0.003551   Mean   : 0.003248   Mean   :-0.00343  
    ##  3rd Qu.: 0.576105   3rd Qu.: 0.490104   3rd Qu.: 0.644332   3rd Qu.: 0.32523  
    ##  Max.   : 6.232606   Max.   :15.301186   Max.   : 3.286473   Max.   :11.26777  
    ##     LBXBAPCT            LBDLYMNO           LBDMONO            LBDNENO         
    ##  Min.   :-1.450297   Min.   :-1.38144   Min.   :-2.25940   Min.   :-2.389857  
    ##  1st Qu.:-0.594351   1st Qu.:-0.38795   1st Qu.:-0.77685   1st Qu.:-0.675187  
    ##  Median :-0.166378   Median :-0.08226   Median :-0.28266   Median :-0.143048  
    ##  Mean   : 0.000091   Mean   : 0.00096   Mean   : 0.00045   Mean   : 0.004022  
    ##  3rd Qu.: 0.261595   3rd Qu.: 0.26164   3rd Qu.: 0.21152   3rd Qu.: 0.507344  
    ##  Max.   :14.170714   Max.   :52.64929   Max.   :24.42652   Max.   : 9.612833  
    ##     LBDEONO             LBDBANO            LBXRBCSI             LBXHGB         
    ##  Min.   :-1.193240   Min.   :-0.63180   Min.   :-4.304403   Min.   :-4.295208  
    ##  1st Qu.:-0.627569   1st Qu.:-0.63180   1st Qu.:-0.645908   1st Qu.:-0.645680  
    ##  Median :-0.061898   Median :-0.63180   Median :-0.022757   Median :-0.005412  
    ##  Mean   :-0.002211   Mean   : 0.00084   Mean   :-0.002254   Mean   :-0.001643  
    ##  3rd Qu.: 0.503773   3rd Qu.: 0.95498   3rd Qu.: 0.660698   3rd Qu.: 0.698883  
    ##  Max.   :15.211214   Max.   :32.69060   Max.   : 4.560413   Max.   : 3.516062  
    ##      LBXHCT             LBXMCVSI            LBXMCHSI            LBXMC          
    ##  Min.   :-3.957151   Min.   :-5.184804   Min.   :-5.05278   Min.   :-4.200890  
    ##  1st Qu.:-0.656812   1st Qu.:-0.465667   1st Qu.:-0.46459   1st Qu.:-0.605236  
    ##  Median : 0.003256   Median : 0.064573   Median : 0.08769   Median : 0.011162  
    ##  Mean   :-0.001733   Mean   : 0.000838   Mean   : 0.00078   Mean   :-0.000272  
    ##  3rd Qu.: 0.710471   3rd Qu.: 0.594813   3rd Qu.: 0.59749   3rd Qu.: 0.627560  
    ##  Max.   : 3.539333   Max.   : 6.586526   Max.   :12.96013   Max.   : 4.736879  
    ##      LBXRDW             LBXPLTSI            LBXMPSI         
    ##  Min.   :-1.701561   Min.   :-3.459410   Min.   :-3.191765  
    ##  1st Qu.:-0.548216   1st Qu.:-0.613750   1st Qu.:-0.680825  
    ##  Median :-0.218689   Median :-0.096357   Median :-0.082982  
    ##  Mean   :-0.001096   Mean   : 0.001965   Mean   :-0.001788  
    ##  3rd Qu.: 0.275602   3rd Qu.: 0.492896   3rd Qu.: 0.514861  
    ##  Max.   :20.541518   Max.   :10.510194   Max.   : 5.775879

Impute the rest of missing values in the factor variables:

For the categorical factor variables, I will impute then with a median.
Since the breakdown is quite evident in the majority of the factor
features, these imputation should not cause significant bias - majority
of the categorical features are heavily swayed toward one or another
category.

    # factor variables added 
    is.fact <- sapply(full.df, is.factor)
    factor.full<- full.df[!badrow, is.fact]

    knn.df<- cbind(factor.full, full_imputed_df)

### Impute the categorical values:

    mode <- function(x) {
        ux <- unique(x)
        ux[which.max(tabulate(match(x, ux)))]
    }

    knn.df<- knn.df %>%  mutate_if(is.factor, 
                                   funs(replace(.,is.na(.), mode(na.omit(.)))))
    #BPAEN1 only has one value type, I am going to omit that feature 
    knn.df<- knn.df %>% dplyr::select(-BPAEN1)
    summary(knn.df)

    ##  PEASCST1 BPQ150A  BPQ150B  BPQ150C  BPQ150D  BPAARM   BPACSZ   BPXPULS 
    ##  1:6240   1:1423   1:   3   1:  45   1: 127   1:6296   2:  94   1:6175  
    ##  2:   9   2:4911   2:6331   2:6289   2:6207   2:  33   3:1898   2: 159  
    ##  3:  85                                       8:   5   4:3405           
    ##                                                        5: 937           
    ##                                                                         
    ##                                                                         
    ##  BPXPTY   BPAEN2   BPAEN3   HSD010_re    PEASCTM1             BPXPLS         
    ##  1:6330   1:  62   1:  56   0:1420    Min.   :-3.745598   Min.   :-2.657940  
    ##  2:   4   2:6272   2:6278   1:4914    1st Qu.:-0.681719   1st Qu.:-0.769549  
    ##                                       Median :-0.186091   Median :-0.140085  
    ##                                       Mean   : 0.000279   Mean   :-0.000769  
    ##                                       3rd Qu.: 0.543833   3rd Qu.: 0.646744  
    ##                                       Max.   : 3.823985   Max.   :11.819724  
    ##      BPXML1              BPXSY1              BPXDI1              BPXSY2        
    ##  Min.   :-2.030578   Min.   :-2.368885   Min.   :-3.821046   Min.   :-2.49253  
    ##  1st Qu.:-0.449175   1st Qu.:-0.694813   1st Qu.:-0.571881   1st Qu.:-0.64524  
    ##  Median :-0.449175   Median :-0.171666   Median : 0.077952   Median :-0.21058  
    ##  Mean   :-0.000617   Mean   : 0.009332   Mean   : 0.007978   Mean   :-0.00551  
    ##  3rd Qu.: 0.605094   3rd Qu.: 0.486628   3rd Qu.: 0.619480   3rd Qu.: 0.44141  
    ##  Max.   : 4.822169   Max.   : 5.582955   Max.   : 3.814492   Max.   : 5.00530  
    ##      BPXDI2              BPXSY3              BPXDI3         
    ##  Min.   :-3.818122   Min.   :-2.600810   Min.   :-3.789903  
    ##  1st Qu.:-0.661804   1st Qu.:-0.709806   1st Qu.:-0.695703  
    ##  Median : 0.072856   Median :-0.153629   Median :-0.044293  
    ##  Mean   :-0.004383   Mean   :-0.008757   Mean   :-0.007668  
    ##  3rd Qu.: 0.589839   3rd Qu.: 0.402548   3rd Qu.: 0.607118  
    ##  Max.   : 3.691738   Max.   : 4.963203   Max.   : 3.701318  
    ##     LBXWBCSI            LBXLYPCT            LBXMOPCT        
    ##  Min.   :-2.370928   Min.   :-2.990976   Min.   :-3.024391  
    ##  1st Qu.:-0.602827   1st Qu.:-0.645028   1st Qu.:-0.639555  
    ##  Median :-0.109403   Median :-0.049958   Median :-0.095645  
    ##  Mean   : 0.002487   Mean   :-0.001843   Mean   :-0.003551  
    ##  3rd Qu.: 0.445698   3rd Qu.: 0.576105   3rd Qu.: 0.490104  
    ##  Max.   :31.222997   Max.   : 6.232606   Max.   :15.301186  
    ##     LBXNEPCT            LBXEOPCT           LBXBAPCT            LBDLYMNO       
    ##  Min.   :-5.156229   Min.   :-1.26320   Min.   :-1.450297   Min.   :-1.38144  
    ##  1st Qu.:-0.590691   1st Qu.:-0.60135   1st Qu.:-0.594351   1st Qu.:-0.38795  
    ##  Median : 0.052128   Median :-0.24837   Median :-0.166378   Median :-0.08226  
    ##  Mean   : 0.003248   Mean   :-0.00343   Mean   : 0.000091   Mean   : 0.00096  
    ##  3rd Qu.: 0.644332   3rd Qu.: 0.32523   3rd Qu.: 0.261595   3rd Qu.: 0.26164  
    ##  Max.   : 3.286473   Max.   :11.26777   Max.   :14.170714   Max.   :52.64929  
    ##     LBDMONO            LBDNENO             LBDEONO             LBDBANO        
    ##  Min.   :-2.25940   Min.   :-2.389857   Min.   :-1.193240   Min.   :-0.63180  
    ##  1st Qu.:-0.77685   1st Qu.:-0.675187   1st Qu.:-0.627569   1st Qu.:-0.63180  
    ##  Median :-0.28266   Median :-0.143048   Median :-0.061898   Median :-0.63180  
    ##  Mean   : 0.00045   Mean   : 0.004022   Mean   :-0.002211   Mean   : 0.00084  
    ##  3rd Qu.: 0.21152   3rd Qu.: 0.507344   3rd Qu.: 0.503773   3rd Qu.: 0.95498  
    ##  Max.   :24.42652   Max.   : 9.612833   Max.   :15.211214   Max.   :32.69060  
    ##     LBXRBCSI             LBXHGB              LBXHCT         
    ##  Min.   :-4.304403   Min.   :-4.295208   Min.   :-3.957151  
    ##  1st Qu.:-0.645908   1st Qu.:-0.645680   1st Qu.:-0.656812  
    ##  Median :-0.022757   Median :-0.005412   Median : 0.003256  
    ##  Mean   :-0.002254   Mean   :-0.001643   Mean   :-0.001733  
    ##  3rd Qu.: 0.660698   3rd Qu.: 0.698883   3rd Qu.: 0.710471  
    ##  Max.   : 4.560413   Max.   : 3.516062   Max.   : 3.539333  
    ##     LBXMCVSI            LBXMCHSI            LBXMC               LBXRDW         
    ##  Min.   :-5.184804   Min.   :-5.05278   Min.   :-4.200890   Min.   :-1.701561  
    ##  1st Qu.:-0.465667   1st Qu.:-0.46459   1st Qu.:-0.605236   1st Qu.:-0.548216  
    ##  Median : 0.064573   Median : 0.08769   Median : 0.011162   Median :-0.218689  
    ##  Mean   : 0.000838   Mean   : 0.00078   Mean   :-0.000272   Mean   :-0.001096  
    ##  3rd Qu.: 0.594813   3rd Qu.: 0.59749   3rd Qu.: 0.627560   3rd Qu.: 0.275602  
    ##  Max.   : 6.586526   Max.   :12.96013   Max.   : 4.736879   Max.   :20.541518  
    ##     LBXPLTSI            LBXMPSI         
    ##  Min.   :-3.459410   Min.   :-3.191765  
    ##  1st Qu.:-0.613750   1st Qu.:-0.680825  
    ##  Median :-0.096357   Median :-0.082982  
    ##  Mean   : 0.001965   Mean   :-0.001788  
    ##  3rd Qu.: 0.492896   3rd Qu.: 0.514861  
    ##  Max.   :10.510194   Max.   : 5.775879

Now this dataset contains fully imputed values. I will double check the
distribution to see if I need not further apply any transformation.

    knn.df %>% dplyr::select_if(is.numeric) %>%  
      gather(cols, value) %>% 
      ggplot(aes(x = value)) +
      geom_histogram() + 
      facet_wrap(.~cols, scale = "free") +
      ggtitle("Distributions of the numeric\n values in the kNN imputed full dataset" ) + 
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  +
      theme(plot.title = element_text(hjust = 0.5)) +
      theme(axis.text.y = element_text(size = 5))

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-20-1.png)

Multicolinearity
----------------

    knn.df %>%  dplyr::select_if(is.factor) %>% names

    ##  [1] "PEASCST1"  "BPQ150A"   "BPQ150B"   "BPQ150C"   "BPQ150D"   "BPAARM"   
    ##  [7] "BPACSZ"    "BPXPULS"   "BPXPTY"    "BPAEN2"    "BPAEN3"    "HSD010_re"

    knn.df.dummy<- fastDummies::dummy_columns(knn.df, 
                                              select_columns= c("PEASCST1",  "BPQ150A" ,  "BPQ150B" , 
                                                                "BPQ150C"  , "BPQ150D",  "BPAARM"  , 
                                                                "BPACSZ", "BPXPULS",   "BPXPTY" ,  
                                                                "BPAEN1",    "BPAEN2" ,   "BPAEN3"), 
                                              remove_first_dummy = T, 
                                              remove_selected_columns = T)

    ## Warning in fastDummies::dummy_columns(knn.df, select_columns = c("PEASCST1", : NOTE: The following select_columns input(s) is not a column in data.
    ##  

    knn.df.dummy %>% dplyr::select(-HSD010_re) %>% 
      cor(., method ="spearman") %>% 
      corrplot(., tl.cex = 0.5, title = "Correlation plot of all variables in full dataset")

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-22-1.png)

This correlation plot demostrates us some expcted correlation values -
first, the three repeats of blood pressure have obvious positive
correlation. Since they are taken during the same survey, they are
expected to have positive correlation. The three blood pressure measures
also have a related question about diastolic blood pressure - epxcted
positive correlation as well. Nother high level of correlation is seen
between LBXLYPCT (Lymphocyte %) and LBXNEPCT (Segmented neutrophils %).
Although the correlation is significant, these are significant values. I
could omit one of the columns from future model building.

Another correlation is seen between LBXWBCSI and LBDNENO, which mackes
sense since LBDNENO is a derived feature from the following expression:

LBDNENO = LBXWBCSI \* LBXNEPCT /100 (round to 1 decimal))

Next, LBXEOPCT (Eosinophil %) is highly correlated with LBDEONO, whihc
is also a derived feature in this case:

LBDEONO = LBXWBCSI \* LBXEOPCT/100

Other derived feature include:

LBDLYMNO = LBXWBCSI \* LBXLYPCT/100 LBDMONO = LBXWBCSI \* LBXMOPCT/100
LBDBANO = LBXWBCSI \* LBXBAPCT/100

Overall, the features which make up these derived feautres could be
exluded from frurther analysis but I am going to give it a try.

Importantly, the response variable is not obviously (=significantly)
correlated with any features in the dataset.

Create data partition:
----------------------

Using 75% of the data for training:

    set.seed(123)
    trainIndex <- createDataPartition(knn.df$HSD010_re, p = .75, 
                                      list = FALSE, 
                                      times = 1)

    (length(trainIndex)/(dim(knn.df)[1]))*100

    ## [1] 75.00789

    knn.train.df<- knn.df[trainIndex, ]
    knn.test.df<- knn.df[-trainIndex, ]
    knn.test.df.y<- knn.test.df %>% dplyr::select( HSD010_re)
    knn.test.df<- knn.test.df %>% dplyr::select(-HSD010_re)

Model application:
------------------

### Logistic regression model

classificaiotn of patients into healthy/ non healthy self reported
answers: logistic regression does not assume the residuals are normally
distributed nor that the variance is constant

    glm.m1<- glm(HSD010_re~., data = knn.train.df, family = "binomial")
    summary(glm.m1)

    ## 
    ## Call:
    ## glm(formula = HSD010_re ~ ., family = "binomial", data = knn.train.df)
    ## 
    ## Deviance Residuals: 
    ##     Min       1Q   Median       3Q      Max  
    ## -2.3262   0.4379   0.5906   0.7151   1.6774  
    ## 
    ## Coefficients:
    ##              Estimate Std. Error z value Pr(>|z|)    
    ## (Intercept) -12.63680  378.42162  -0.033  0.97336    
    ## PEASCST12    -0.25059    1.30878  -0.191  0.84816    
    ## PEASCST13    -0.09620    0.30696  -0.313  0.75397    
    ## BPQ150A2     -0.17188    0.08997  -1.910  0.05608 .  
    ## BPQ150B2     14.65240  378.42067   0.039  0.96911    
    ## BPQ150C2      0.12299    0.41533   0.296  0.76714    
    ## BPQ150D2      0.46108    0.23085   1.997  0.04580 *  
    ## BPAARM2      -0.60664    0.46011  -1.318  0.18735    
    ## BPAARM8      -1.81441    1.80023  -1.008  0.31352    
    ## BPACSZ3       0.10846    0.32892   0.330  0.74159    
    ## BPACSZ4      -0.08923    0.32645  -0.273  0.78459    
    ## BPACSZ5      -0.50158    0.33474  -1.498  0.13402    
    ## BPXPULS2     -0.05684    0.21867  -0.260  0.79491    
    ## BPXPTY2      11.56439  366.70189   0.032  0.97484    
    ## BPAEN22      -0.49152    0.46048  -1.067  0.28578    
    ## BPAEN32      -0.53882    0.48908  -1.102  0.27058    
    ## PEASCTM1      0.05793    0.03955   1.465  0.14303    
    ## BPXPLS       -0.04874    0.03838  -1.270  0.20412    
    ## BPXML1       -0.15142    0.09436  -1.605  0.10855    
    ## BPXSY1       -0.19680    0.13223  -1.488  0.13667    
    ## BPXDI1        0.15596    0.08132   1.918  0.05512 .  
    ## BPXSY2        0.13944    0.14187   0.983  0.32567    
    ## BPXDI2       -0.26378    0.09442  -2.794  0.00521 ** 
    ## BPXSY3       -0.12724    0.12911  -0.985  0.32438    
    ## BPXDI3        0.15938    0.08887   1.793  0.07291 .  
    ## LBXWBCSI     -0.55177    0.49688  -1.110  0.26680    
    ## LBXLYPCT      4.36749    4.22718   1.033  0.30151    
    ## LBXMOPCT      1.30808    1.15979   1.128  0.25938    
    ## LBXNEPCT      5.07447    4.77481   1.063  0.28789    
    ## LBXEOPCT      1.12773    1.10317   1.022  0.30666    
    ## LBXBAPCT      0.23720    0.23471   1.011  0.31220    
    ## LBDLYMNO      0.39095    0.28968   1.350  0.17715    
    ## LBDMONO       0.06972    0.11811   0.590  0.55499    
    ## LBDNENO       0.18490    0.36362   0.508  0.61111    
    ## LBDEONO      -0.01878    0.11778  -0.159  0.87329    
    ## LBDBANO      -0.04301    0.06591  -0.653  0.51405    
    ## LBXRBCSI      0.26839    0.39898   0.673  0.50115    
    ## LBXHGB       -0.80405    0.91044  -0.883  0.37716    
    ## LBXHCT        0.64956    1.00259   0.648  0.51706    
    ## LBXMCVSI     -1.05302    0.88667  -1.188  0.23499    
    ## LBXMCHSI      1.34192    0.98833   1.358  0.17454    
    ## LBXMC        -0.28206    0.35661  -0.791  0.42898    
    ## LBXRDW       -0.28920    0.04697  -6.158 7.38e-10 ***
    ## LBXPLTSI     -0.02191    0.04434  -0.494  0.62114    
    ## LBXMPSI      -0.04921    0.04124  -1.193  0.23271    
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 5056.3  on 4750  degrees of freedom
    ## Residual deviance: 4742.8  on 4706  degrees of freedom
    ## AIC: 4832.8
    ## 
    ## Number of Fisher Scoring iterations: 12

I am going to perform step wise logistic regression based on AIC:

    best.glm.m1<- glm(HSD010_re~., data = knn.train.df, family = "binomial") %>%
      stepAIC(trace = FALSE)
    summary(best.glm.m1)

    ## 
    ## Call:
    ## glm(formula = HSD010_re ~ BPQ150A + BPQ150B + BPQ150D + BPAARM + 
    ##     BPACSZ + PEASCTM1 + BPXML1 + BPXSY1 + BPXDI1 + BPXDI2 + BPXDI3 + 
    ##     LBXWBCSI + LBXMOPCT + LBXNEPCT + LBDLYMNO + LBXHGB + LBXHCT + 
    ##     LBXMCVSI + LBXMCHSI + LBXRDW, family = "binomial", data = knn.train.df)
    ## 
    ## Deviance Residuals: 
    ##     Min       1Q   Median       3Q      Max  
    ## -2.3343   0.4498   0.5933   0.7163   1.7119  
    ## 
    ## Coefficients:
    ##              Estimate Std. Error z value Pr(>|z|)    
    ## (Intercept) -12.61128  229.61083  -0.055 0.956199    
    ## BPQ150A2     -0.18340    0.08935  -2.053 0.040108 *  
    ## BPQ150B2     13.70500  229.61050   0.060 0.952404    
    ## BPQ150D2      0.49146    0.22899   2.146 0.031855 *  
    ## BPAARM2      -0.60394    0.45643  -1.323 0.185776    
    ## BPAARM8      -1.96501    1.23426  -1.592 0.111371    
    ## BPACSZ3       0.10862    0.32864   0.331 0.740999    
    ## BPACSZ4      -0.09412    0.32575  -0.289 0.772639    
    ## BPACSZ5      -0.51814    0.33449  -1.549 0.121369    
    ## PEASCTM1      0.06528    0.03883   1.681 0.092713 .  
    ## BPXML1       -0.14418    0.09186  -1.570 0.116524    
    ## BPXSY1       -0.19106    0.08949  -2.135 0.032752 *  
    ## BPXDI1        0.15542    0.08052   1.930 0.053589 .  
    ## BPXDI2       -0.25892    0.09396  -2.756 0.005858 ** 
    ## BPXDI3        0.15236    0.08808   1.730 0.083684 .  
    ## LBXWBCSI     -0.35667    0.08173  -4.364 1.28e-05 ***
    ## LBXMOPCT      0.16608    0.04676   3.551 0.000383 ***
    ## LBXNEPCT      0.20720    0.07607   2.724 0.006456 ** 
    ## LBDLYMNO      0.33797    0.11164   3.027 0.002467 ** 
    ## LBXHGB       -1.27607    0.82331  -1.550 0.121160    
    ## LBXHCT        1.35175    0.76945   1.757 0.078959 .  
    ## LBXMCVSI     -0.85201    0.48908  -1.742 0.081497 .  
    ## LBXMCHSI      0.89749    0.59158   1.517 0.129238    
    ## LBXRDW       -0.28712    0.04469  -6.425 1.32e-10 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 5056.3  on 4750  degrees of freedom
    ## Residual deviance: 4755.4  on 4727  degrees of freedom
    ## AIC: 4803.4
    ## 
    ## Number of Fisher Scoring iterations: 11

Reamaining features include systolic (1st) and distolic (1,2) blood
pressures,but also some features that do no look significant in the
model, such as BPACSZ(number), which correspond to the “codded cuff size
of the person taking the survee).Looks likes BPAEN2 ( correponds to
enhacement used for the second blood pressue take) is also non
significant in this model with asnwer 2 =”No".

    probabilities <- best.glm.m1 %>% predict(knn.test.df, type = "response")
    predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
    mean(predicted.classes == knn.test.df.y)

    ## [1] 0.7770057

    pre.df<- as.data.frame(predicted.classes)
    pre.df$predicted.classes<- as.factor(pre.df$predicted.classes)
    confusionMatrix(pre.df$predicted.classes, knn.test.df.y$HSD010_re)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction    0    1
    ##          0   20   18
    ##          1  335 1210
    ##                                           
    ##                Accuracy : 0.777           
    ##                  95% CI : (0.7557, 0.7973)
    ##     No Information Rate : 0.7757          
    ##     P-Value [Acc > NIR] : 0.4662          
    ##                                           
    ##                   Kappa : 0.0611          
    ##                                           
    ##  Mcnemar's Test P-Value : <2e-16          
    ##                                           
    ##             Sensitivity : 0.05634         
    ##             Specificity : 0.98534         
    ##          Pos Pred Value : 0.52632         
    ##          Neg Pred Value : 0.78317         
    ##              Prevalence : 0.22426         
    ##          Detection Rate : 0.01263         
    ##    Detection Prevalence : 0.02401         
    ##       Balanced Accuracy : 0.52084         
    ##                                           
    ##        'Positive' Class : 0               
    ## 

The model has a hihg specificity but a very low sensitivty.

    car::vif(best.glm.m1)

    ##                GVIF Df GVIF^(1/(2*Df))
    ## BPQ150A    1.008785  1        1.004383
    ## BPQ150B    1.000000  1        1.000000
    ## BPQ150D    1.032534  1        1.016137
    ## BPAARM     1.015127  2        1.003760
    ## BPACSZ     1.150077  3        1.023578
    ## PEASCTM1   1.180568  1        1.086540
    ## BPXML1     7.348058  1        2.710730
    ## BPXSY1     7.087255  1        2.662190
    ## BPXDI1     5.249474  1        2.291173
    ## BPXDI2     6.850822  1        2.617408
    ## BPXDI3     6.063843  1        2.462487
    ## LBXWBCSI   4.543704  1        2.131597
    ## LBXMOPCT   1.473871  1        1.214031
    ## LBXNEPCT   4.266142  1        2.065464
    ## LBDLYMNO   4.907444  1        2.215275
    ## LBXHGB   531.219078  1       23.048190
    ## LBXHCT   453.086736  1       21.285834
    ## LBXMCVSI 197.629308  1       14.058069
    ## LBXMCHSI 281.235430  1       16.770075
    ## LBXRDW     1.613427  1        1.270208

VIF is a good method of exploration of multicolinearity, Variance
infaltion factor is considered as large &gt;5 and extremely significant
&gt;10. I have a couple features in the model with a high VIF od almost
10 (~9.3) and I will exlude them from the model.

Exploratin of logistic model with Cook’s distance values: Thes one
demostrated here have significant effect on the data and make up the
significant outliers.

    plot(best.glm.m1, which = 4, id.n = 5)

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-28-1.png)

AUC and ROC:

    library(pROC)
    pre.df$predicted.classes<- as.numeric(pre.df$predicted.classes)
    knn.test.df.y$HSD010_re<- as.numeric(knn.test.df.y$HSD010_re)
    roccurve.glm <- roc(knn.test.df.y$HSD010_re ~ pre.df$predicted.classes)

    ## Setting levels: control = 1, case = 2

    ## Setting direction: controls < cases

    # shows very bad performance:
    plot(roccurve.glm)

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-29-1.png)

As expected, a very low value of the AUC for such a poor performace in
terms of sensitivity of the model.

    auc(roccurve.glm)

    ## Area under the curve: 0.5208

### Classification with rules:

    knn.train.tree.y<- knn.train.df %>% dplyr::select(HSD010_re)
    knn.train.tree<- knn.train.df %>% dplyr::select(-HSD010_re)

    rules_mod <- C5.0(x = knn.train.tree, 
                      y = knn.train.tree.y$HSD010_re, rules = TRUE)
    summary(rules_mod)

    ## 
    ## Call:
    ## C5.0.default(x = knn.train.tree, y = knn.train.tree.y$HSD010_re, rules = TRUE)
    ## 
    ## 
    ## C5.0 [Release 2.07 GPL Edition]      Sun Aug 23 12:03:50 2020
    ## -------------------------------
    ## 
    ## Class specified by attribute `outcome'
    ## 
    ## Read 4751 cases (41 attributes) from undefined.data
    ## 
    ## Rules:
    ## 
    ## Rule 1: (9, lift 4.1)
    ##  BPACSZ = 3
    ##  BPXSY3 > 2.182316
    ##  LBXRDW > -0.0539251
    ##  LBXRDW <= 0.3991746
    ##  ->  class 0  [0.909]
    ## 
    ## Rule 2: (5, lift 3.8)
    ##  BPACSZ = 3
    ##  BPXSY3 > 2.182316
    ##  LBXMC <= -1.067534
    ##  LBXRDW > -0.0539251
    ##  ->  class 0  [0.857]
    ## 
    ## Rule 3: (12/2, lift 3.5)
    ##  BPQ150A = 1
    ##  BPXSY3 > 2.182316
    ##  LBXRDW > -0.0539251
    ##  ->  class 0  [0.786]
    ## 
    ## Rule 4: (50/18, lift 2.8)
    ##  LBXNEPCT > 1.980587
    ##  LBXRDW > -0.0539251
    ##  ->  class 0  [0.635]
    ## 
    ## Rule 5: (94/41, lift 2.5)
    ##  BPXSY3 > 2.182316
    ##  LBXRDW > -0.0539251
    ##  ->  class 0  [0.563]
    ## 
    ## Rule 6: (2848/490, lift 1.1)
    ##  LBXRDW <= -0.0539251
    ##  ->  class 1  [0.828]
    ## 
    ## Rule 7: (1370/252, lift 1.1)
    ##  BPACSZ = 3
    ##  LBXNEPCT <= 1.980587
    ##  ->  class 1  [0.816]
    ## 
    ## Rule 8: (2200/435, lift 1.0)
    ##  BPACSZ = 4
    ##  LBXNEPCT <= 1.980587
    ##  LBXHCT > -1.01042
    ##  ->  class 1  [0.802]
    ## 
    ## Rule 9: (4513/955, lift 1.0)
    ##  BPXSY3 <= 2.182316
    ##  LBXNEPCT <= 1.980587
    ##  ->  class 1  [0.788]
    ## 
    ## Default class: 1
    ## 
    ## 
    ## Evaluation on training data (4751 cases):
    ## 
    ##          Rules     
    ##    ----------------
    ##      No      Errors
    ## 
    ##       9 1024(21.6%)   <<
    ## 
    ## 
    ##     (a)   (b)    <-classified as
    ##    ----  ----
    ##      72   993    (a): class 0
    ##      31  3655    (b): class 1
    ## 
    ## 
    ##  Attribute usage:
    ## 
    ##   98.32% LBXNEPCT
    ##   96.97% BPXSY3
    ##   75.16% BPACSZ
    ##   62.93% LBXRDW
    ##   46.31% LBXHCT
    ##    0.25% BPQ150A
    ##    0.11% LBXMC
    ## 
    ## 
    ## Time: 0.1 secs

    rules.pred<- predict(rules_mod, newdata = knn.test.df)
    rules.pred<- as.data.frame(rules.pred)
    rules.pred$rules.pred<- as.factor(rules.pred$rules.pred)
    knn.test.df.y$HSD010_re<- ifelse(knn.test.df.y$HSD010_re == 1, 1, 0)
    knn.test.df.y$HSD010_re<- as.factor(knn.test.df.y$HSD010_re)
    confusionMatrix(rules.pred$rules.pred, knn.test.df.y$HSD010_re)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction    0    1
    ##          0   18   12
    ##          1 1210  343
    ##                                           
    ##                Accuracy : 0.228           
    ##                  95% CI : (0.2076, 0.2495)
    ##     No Information Rate : 0.7757          
    ##     P-Value [Acc > NIR] : 1               
    ##                                           
    ##                   Kappa : -0.0087         
    ##                                           
    ##  Mcnemar's Test P-Value : <2e-16          
    ##                                           
    ##             Sensitivity : 0.01466         
    ##             Specificity : 0.96620         
    ##          Pos Pred Value : 0.60000         
    ##          Neg Pred Value : 0.22086         
    ##              Prevalence : 0.77574         
    ##          Detection Rate : 0.01137         
    ##    Detection Prevalence : 0.01895         
    ##       Balanced Accuracy : 0.49043         
    ##                                           
    ##        'Positive' Class : 0               
    ## 

Very high specificity but very low sensitivity - true positives are
barely recognized. Although the accuracy is good, we have a large issue
of being biased towards always predicting 1 isntead of 0 and because
that dataset is unbalances, this issue creates high accuracy but low
sensitivity.

    prop.table(table(knn.df$HSD010_re))

    ## 
    ##         0         1 
    ## 0.2241869 0.7758131

**ROC and AUC**:

    rules.pred$rules.pred<- as.numeric(rules.pred$rules.pred)
    knn.test.df.y$HSD010_re<- as.numeric(knn.test.df.y$HSD010_re)
    roccurve.C5 <- roc(knn.test.df.y$HSD010_re ~ pre.df$predicted.classes)

    ## Setting levels: control = 1, case = 2

    ## Setting direction: controls < cases

    # shows very bad performance:
    plot(roccurve.C5)

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-33-1.png)

Expected low value of AUC for low sensitivity of the model.

    auc(roccurve.C5)

    ## Area under the curve: 0.4792

### Neural Net

Values for neural net need to be normalized, in this case this was done
with z score normalization before kNN imputation.

    NN_train<- knn.df.dummy[trainIndex, ]
    NN_test<- knn.df.dummy[-trainIndex, ]
    NN_test_y<- NN_test %>% dplyr::select(HSD010_re)
    NN_test_y$HSD010_re<- ifelse(NN_test_y$HSD010_re == 1, 1, 0)
    NN_test <- NN_test %>% dplyr::select(-HSD010_re)
    NN_train$HSD010_re<- ifelse(NN_train$HSD010_re==1, 1,0)

    n <- names(NN_train)
    f <- as.formula(paste("HSD010_re~", paste(n[!n %in% "HSD010_re"], collapse = " + ")))
    f

    ## HSD010_re ~ PEASCTM1 + BPXPLS + BPXML1 + BPXSY1 + BPXDI1 + BPXSY2 + 
    ##     BPXDI2 + BPXSY3 + BPXDI3 + LBXWBCSI + LBXLYPCT + LBXMOPCT + 
    ##     LBXNEPCT + LBXEOPCT + LBXBAPCT + LBDLYMNO + LBDMONO + LBDNENO + 
    ##     LBDEONO + LBDBANO + LBXRBCSI + LBXHGB + LBXHCT + LBXMCVSI + 
    ##     LBXMCHSI + LBXMC + LBXRDW + LBXPLTSI + LBXMPSI + PEASCST1_2 + 
    ##     PEASCST1_3 + BPQ150A_2 + BPQ150B_2 + BPQ150C_2 + BPQ150D_2 + 
    ##     BPAARM_2 + BPAARM_8 + BPACSZ_3 + BPACSZ_4 + BPACSZ_5 + BPXPULS_2 + 
    ##     BPXPTY_2 + BPAEN2_2 + BPAEN3_2

    # linear.output=FALSE specifies that I want to do classification and not regression. 
    neu.class1<- neuralnet(f, data = NN_train, linear.output= FALSE, stepmax=1e6)

    plot(neu.class1, rep = "best")

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-35-1.png)

    #  error of the neural network model, along with the weights between the inputs, hidden layers, and outputs:
    neu.class1$result.matrix

    ##                                 [,1]
    ## error                   3.841075e+02
    ## reached.threshold       9.877821e-03
    ## steps                   6.356000e+04
    ## Intercept.to.1layhid1   1.372557e+00
    ## PEASCTM1.to.1layhid1    2.779825e-02
    ## BPXPLS.to.1layhid1     -1.133753e-02
    ## BPXML1.to.1layhid1     -4.964615e-02
    ## BPXSY1.to.1layhid1     -1.906162e-01
    ## BPXDI1.to.1layhid1      7.473494e-02
    ## BPXSY2.to.1layhid1      1.332820e-01
    ## BPXDI2.to.1layhid1     -1.572347e-01
    ## BPXSY3.to.1layhid1     -1.222286e-01
    ## BPXDI3.to.1layhid1      1.116406e-01
    ## LBXWBCSI.to.1layhid1   -2.892211e-01
    ## LBXLYPCT.to.1layhid1    1.272672e+00
    ## LBXMOPCT.to.1layhid1    4.411467e-01
    ## LBXNEPCT.to.1layhid1    1.464426e+00
    ## LBXEOPCT.to.1layhid1    2.837328e-01
    ## LBXBAPCT.to.1layhid1    7.524654e-02
    ## LBDLYMNO.to.1layhid1    2.110171e-01
    ## LBDMONO.to.1layhid1     1.572934e-02
    ## LBDNENO.to.1layhid1     1.004065e-01
    ## LBDEONO.to.1layhid1     8.751281e-03
    ## LBDBANO.to.1layhid1    -5.213989e-02
    ## LBXRBCSI.to.1layhid1    6.336822e-01
    ## LBXHGB.to.1layhid1     -1.178416e-02
    ## LBXHCT.to.1layhid1     -4.774246e-01
    ## LBXMCVSI.to.1layhid1   -9.340535e-01
    ## LBXMCHSI.to.1layhid1    1.512534e+00
    ## LBXMC.to.1layhid1      -5.157841e-01
    ## LBXRDW.to.1layhid1     -1.689838e-01
    ## LBXPLTSI.to.1layhid1    4.731577e-03
    ## LBXMPSI.to.1layhid1    -2.691747e-02
    ## PEASCST1_2.to.1layhid1  1.327589e+00
    ## PEASCST1_3.to.1layhid1 -1.649899e-01
    ## BPQ150A_2.to.1layhid1  -1.165058e-01
    ## BPQ150B_2.to.1layhid1   2.443431e+00
    ## BPQ150C_2.to.1layhid1  -1.323042e-01
    ## BPQ150D_2.to.1layhid1   3.794716e-01
    ## BPAARM_2.to.1layhid1   -3.784176e-01
    ## BPAARM_8.to.1layhid1   -2.526549e+00
    ## BPACSZ_3.to.1layhid1    1.163557e-01
    ## BPACSZ_4.to.1layhid1   -7.608462e-03
    ## BPACSZ_5.to.1layhid1   -3.435453e-01
    ## BPXPULS_2.to.1layhid1  -1.074267e-02
    ## BPXPTY_2.to.1layhid1    3.622488e+01
    ## BPAEN2_2.to.1layhid1   -1.144722e+00
    ## BPAEN3_2.to.1layhid1   -1.288449e+00
    ## Intercept.to.HSD010_re -5.338495e+00
    ## 1layhid1.to.HSD010_re   8.201029e+00

    neu.class1.pred<- compute(neu.class1, NN_test)
    cor(NN_test_y$HSD010_re, neu.class1.pred$net.result)

    ##           [,1]
    ## [1,] 0.2233453

    prob <- neu.class1.pred$net.result
    pred <- ifelse(prob>0.5, 1, 0)
    pred<- as.data.frame(pred)
    pred$V1<- as.factor(pred$V1)
    NN_test_y$y_fac<-as.factor(NN_test_y$HSD010_re)

    confusionMatrix(data = pred$V1, reference = NN_test_y$y_fac)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction    0    1
    ##          0   20   21
    ##          1  335 1207
    ##                                           
    ##                Accuracy : 0.7751          
    ##                  95% CI : (0.7537, 0.7955)
    ##     No Information Rate : 0.7757          
    ##     P-Value [Acc > NIR] : 0.5382          
    ##                                           
    ##                   Kappa : 0.0572          
    ##                                           
    ##  Mcnemar's Test P-Value : <2e-16          
    ##                                           
    ##             Sensitivity : 0.05634         
    ##             Specificity : 0.98290         
    ##          Pos Pred Value : 0.48780         
    ##          Neg Pred Value : 0.78275         
    ##              Prevalence : 0.22426         
    ##          Detection Rate : 0.01263         
    ##    Detection Prevalence : 0.02590         
    ##       Balanced Accuracy : 0.51962         
    ##                                           
    ##        'Positive' Class : 0               
    ## 

Interestingly, neural net performs not better than the rest of the
algorithms.It consistently calls only one category - the overrepresented
one.

### SVM Model

    knn.svm_1<- ksvm(HSD010_re~., data = knn.train.df, kernel = "rbfdot")

    knn.pred.svm_1<- predict(knn.svm_1, newdata = knn.test.df)

    # confusion matrix:
    knn.pred.svm_1<- as.data.frame(knn.pred.svm_1)
    knn.pred.svm_1$knn.pred.svm_1<- as.factor(knn.pred.svm_1$knn.pred.svm_1)

    #make sure that the reference response is in the correct format:
    knn.test.df.y$HSD010_re<- ifelse(knn.test.df.y$HSD010_re == 1, 1, 0)
    knn.test.df.y$HSD010_re<- as.factor(knn.test.df.y$HSD010_re)

    confusionMatrix(knn.pred.svm_1$knn.pred.svm_1,knn.test.df.y$HSD010_re)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction    0    1
    ##          0    4    1
    ##          1  351 1227
    ##                                           
    ##                Accuracy : 0.7776          
    ##                  95% CI : (0.7563, 0.7979)
    ##     No Information Rate : 0.7757          
    ##     P-Value [Acc > NIR] : 0.4423          
    ##                                           
    ##                   Kappa : 0.0161          
    ##                                           
    ##  Mcnemar's Test P-Value : <2e-16          
    ##                                           
    ##             Sensitivity : 0.011268        
    ##             Specificity : 0.999186        
    ##          Pos Pred Value : 0.800000        
    ##          Neg Pred Value : 0.777567        
    ##              Prevalence : 0.224258        
    ##          Detection Rate : 0.002527        
    ##    Detection Prevalence : 0.003159        
    ##       Balanced Accuracy : 0.505227        
    ##                                           
    ##        'Positive' Class : 0               
    ## 

In line with previous models, this has a very low sensitivity level,
which becomes a real problem in a real dataset (since we are not able to
confidently call out the true positives). Sensitivity of this model is
almost non existent.

    knn.pred.svm_1$knn.pred.svm_1<- ifelse(knn.pred.svm_1$knn.pred.svm_1 == 0, 0, 1)
    roccurve_svm_1<- roc(knn.test.df.y$HSD010_re, knn.pred.svm_1$knn.pred.svm_1)

    ## Setting levels: control = 0, case = 1

    ## Setting direction: controls < cases

    plot(roccurve_svm_1)

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-40-1.png)

    auc(roccurve_svm_1)

    ## Area under the curve: 0.5052

PCA
---

For PCA analysis, my dataset is already scaled, and I will used the fast
Dummy encoded data for that, as PCA can handle one hot encoded
categorical variables, and does not work with factors (since we are
minimizing the variance of all but teh first few components). Data for
PCA needs to be scaled (knn.df.dummy)

    pca_train<- knn.df.dummy[trainIndex, ] %>% dplyr::select(-HSD010_re)
    pca_test<- knn.df.dummy[-trainIndex, ] %>% dplyr::select(-HSD010_re)


    pca <- prcomp(pca_train)

    # variance :
    pr_var <- pca$sdev^2
    # % of variance 
    prop_varex<- pr_var/sum(pr_var)

    plot(prop_varex, xlab = "Principal Component", ylab = "Proportion of dvariance explained", type = "b")

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-42-1.png)

    # scree plot:
    plot(cumsum(prop_varex), xlab = "Principal component", ylab = "Cumulative Proportion of Variance explined", type = "b")

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-43-1.png)

Based on this screeplot, we can reduce the dfimensions of our dataset
used in the modelling down to 14 components, as more than 90% of the
variance in the dataset is epxlained by them. That is a significant
reduction from 44 features. Further use this first 14 components
(instead of 44) to in the modelling. Data frame for model learning on
the pricnipal component analysis data set:

    # this data set contains my response variable and all the pricniple components. 
    train<- data.frame(HSD010_re = knn.train.df$HSD010_re, pca$x)
    # first column is the reposnes variable, the next 14 are the principal components I am chooosing to keep"
    new_train_pca<- train[,1:15]


    # pca.test has the same information as the normal knn full dummy test set but we need to convert the test data set into the principal component measure, so we use the calculated pca to predict the test eigenvectors. 
    t<- as.data.frame(predict(pca, newdata = pca_test))
    # choose the same principal components as in the train data:
    new_test_pca<- t[,1:14]
    # to check how the classifier worked, we can still use:
    pca_test_y<- knn.df.dummy[-trainIndex, ] %>% dplyr::select(HSD010_re)

### Neural Net with PCA

I am going to see if the neural net performance of non mice imputated
and still imbalanced data will improve upon using PCA:

    n2<- names(new_train_pca)
    f2<- as.formula(paste("HSD010_re~", paste(n2[!n2 %in% "HSD010_re"], collapse = "+")))
    f2

    ## HSD010_re ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + 
    ##     PC10 + PC11 + PC12 + PC13 + PC14

    set.seed(123)
    new_train_pca$HSD010_re<- ifelse(new_train_pca$HSD010_re == 1, 1, 0)
    nn_pca<- neuralnet(f2, new_train_pca, linear.output= FALSE, hidden =4,  stepmax=1e6 )
    plot(nn_pca)

    set.seed(123)
    nn_pca_res<- compute(nn_pca, new_test_pca)
    pca_test_y$HSD010_re<- ifelse(pca_test_y$HSD010_re == 1, 1, 0)

    results_pca_nn<- data.frame(actual = pca_test_y$HSD010_re, prediction = round(nn_pca_res$net.result))
    confusionMatrix(table(results_pca_nn))

    ## Confusion Matrix and Statistics
    ## 
    ##       prediction
    ## actual    0    1
    ##      0   22  333
    ##      1   29 1199
    ##                                           
    ##                Accuracy : 0.7713          
    ##                  95% CI : (0.7498, 0.7918)
    ##     No Information Rate : 0.9678          
    ##     P-Value [Acc > NIR] : 1               
    ##                                           
    ##                   Kappa : 0.0551          
    ##                                           
    ##  Mcnemar's Test P-Value : <2e-16          
    ##                                           
    ##             Sensitivity : 0.43137         
    ##             Specificity : 0.78264         
    ##          Pos Pred Value : 0.06197         
    ##          Neg Pred Value : 0.97638         
    ##              Prevalence : 0.03222         
    ##          Detection Rate : 0.01390         
    ##    Detection Prevalence : 0.22426         
    ##       Balanced Accuracy : 0.60700         
    ##                                           
    ##        'Positive' Class : 0               
    ## 

There has bee a significant improvmenet in neural net performance using
PCA data - accuracy went up to 77%, sensitivity of 55%, and specificity
of 78%.

### SVM with PCA

    set.seed(123)
    # test - new_test_pca
    # test y - pca_test_y
    # train - new_train_pca
    pca.svm<- ksvm(HSD010_re~., data = new_train_pca, kernel = "rbfdot")

    pca.pred.svm_1<- predict(pca.svm, newdata = new_test_pca)

    # confusion matrix:
    pca.pred.svm<- as.data.frame(round(pca.pred.svm_1))

    pca.pred.svm$V1<- as.factor(pca.pred.svm$V1)
    pca_test_y$HSD010_re<- as.factor(pca_test_y$HSD010_re)
    confusionMatrix(pca.pred.svm$V1,pca_test_y$HSD010_re)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction    0    1
    ##          0    1    0
    ##          1  354 1228
    ##                                          
    ##                Accuracy : 0.7764         
    ##                  95% CI : (0.755, 0.7967)
    ##     No Information Rate : 0.7757         
    ##     P-Value [Acc > NIR] : 0.4902         
    ##                                          
    ##                   Kappa : 0.0044         
    ##                                          
    ##  Mcnemar's Test P-Value : <2e-16         
    ##                                          
    ##             Sensitivity : 0.0028169      
    ##             Specificity : 1.0000000      
    ##          Pos Pred Value : 1.0000000      
    ##          Neg Pred Value : 0.7762326      
    ##              Prevalence : 0.2242577      
    ##          Detection Rate : 0.0006317      
    ##    Detection Prevalence : 0.0006317      
    ##       Balanced Accuracy : 0.5014085      
    ##                                          
    ##        'Positive' Class : 0              
    ## 

Still strongly misclassified. Goign further, I am going to take a
different approach to working with my imbalanced data. Using PCA data
for the radial svm does not work. I am going to make a cross validated
version fo svm radial usins this PCA dataset:

##### K fold cv SVM PCA

    new_train_pca$HSD010_re<- as.factor(new_train_pca$HSD010_re)
    fit.control <- caret::trainControl(method = "cv", number = 10)
    svm_pca.fit<- caret::train(HSD010_re~., data = new_train_pca,
                               method = "svmRadial",trControl = fit.control)
    print(svm_pca.fit)

    ## Support Vector Machines with Radial Basis Function Kernel 
    ## 
    ## 4751 samples
    ##   14 predictor
    ##    2 classes: '0', '1' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold) 
    ## Summary of sample sizes: 4276, 4275, 4276, 4275, 4277, 4276, ... 
    ## Resampling results across tuning parameters:
    ## 
    ##   C     Accuracy   Kappa      
    ##   0.25  0.7758373  0.000000000
    ##   0.50  0.7758373  0.000000000
    ##   1.00  0.7766803  0.006854313
    ## 
    ## Tuning parameter 'sigma' was held constant at a value of 0.05396335
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final values used for the model were sigma = 0.05396335 and C = 1.

    svm_pca_pred<- data.frame(pred = predict.train(svm_pca.fit, new_test_pca))
    confusionMatrix(svm_pca_pred$pred, pca_test_y$HSD010_re)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction    0    1
    ##          0    1    0
    ##          1  354 1228
    ##                                          
    ##                Accuracy : 0.7764         
    ##                  95% CI : (0.755, 0.7967)
    ##     No Information Rate : 0.7757         
    ##     P-Value [Acc > NIR] : 0.4902         
    ##                                          
    ##                   Kappa : 0.0044         
    ##                                          
    ##  Mcnemar's Test P-Value : <2e-16         
    ##                                          
    ##             Sensitivity : 0.0028169      
    ##             Specificity : 1.0000000      
    ##          Pos Pred Value : 1.0000000      
    ##          Neg Pred Value : 0.7762326      
    ##              Prevalence : 0.2242577      
    ##          Detection Rate : 0.0006317      
    ##    Detection Prevalence : 0.0006317      
    ##       Balanced Accuracy : 0.5014085      
    ##                                          
    ##        'Positive' Class : 0              
    ## 

Misclassification for one class.

------------------------------------------------------------------------

Different Approach:
-------------------

I am going to use mice to impute the values and have a different
breakdown of the dataset into training and testing to achieve better
classification and avoid the unbalanced class problem as much as
posssible:

### Mice imputation:

badrow (rows with missing values in each feature) will be excluded from
mice imputation as well. The work is picked up at the scaled (z score
normalized), centered, and significant outlier removed dataset
(full.df). This a relatively large dataset, so too many iterations can
be time consuming.

“The mice package assumes a distribution for each variable and imputes
missing variables according to that
distribution.”\[<a href="https://uvastatlab.github.io/2019/05/01/getting-started-with-multiple-imputation-in-r/" class="uri">https://uvastatlab.github.io/2019/05/01/getting-started-with-multiple-imputation-in-r/</a>}\]

    # making sure to omit the categorical feature which has only one factor level and NA values - not informative and prevents effective imputation. 
    full.df<- full.df %>% dplyr::select(-BPAEN1)

    # using mice function, I am going to consider  imputing all rows except for the SEQN - that feature will be omitted going forward overall. This imputation is done using random forest method.
    mice.mis <- mice(full.df[!badrow, !names(full.df) %in% "SEQN"], 
                     method="rf", maxit = 3, print =  FALSE)  

    ## Warning: Number of logged events: 560

    summary(mice.mis)

    ## Class: mids
    ## Number of multiple imputations:  5 
    ## Imputation methods:
    ##    HSD010  PEASCST1  PEASCTM1   BPQ150A   BPQ150B   BPQ150C   BPQ150D    BPAARM 
    ##        ""        ""      "rf"      "rf"      "rf"      "rf"      "rf"      "rf" 
    ##    BPACSZ    BPXPLS   BPXPULS    BPXPTY    BPXML1    BPXSY1    BPXDI1    BPXSY2 
    ##      "rf"      "rf"      "rf"      "rf"      "rf"      "rf"      "rf"      "rf" 
    ##    BPXDI2    BPAEN2    BPXSY3    BPXDI3    BPAEN3  LBXWBCSI  LBXLYPCT  LBXMOPCT 
    ##      "rf"      "rf"      "rf"      "rf"      "rf"      "rf"      "rf"      "rf" 
    ##  LBXNEPCT  LBXEOPCT  LBXBAPCT  LBDLYMNO   LBDMONO   LBDNENO   LBDEONO   LBDBANO 
    ##      "rf"      "rf"      "rf"      "rf"      "rf"      "rf"      "rf"      "rf" 
    ##  LBXRBCSI    LBXHGB    LBXHCT  LBXMCVSI  LBXMCHSI     LBXMC    LBXRDW  LBXPLTSI 
    ##      "rf"      "rf"      "rf"      "rf"      "rf"      "rf"      "rf"      "rf" 
    ##   LBXMPSI HSD010_re 
    ##      "rf"        "" 
    ## PredictorMatrix:
    ##          HSD010 PEASCST1 PEASCTM1 BPQ150A BPQ150B BPQ150C BPQ150D BPAARM BPACSZ
    ## HSD010        0        1        1       1       1       1       1      1      1
    ## PEASCST1      1        0        1       1       1       1       1      1      1
    ## PEASCTM1      1        1        0       1       1       1       1      1      1
    ## BPQ150A       1        1        1       0       1       1       1      1      1
    ## BPQ150B       1        1        1       1       0       1       1      1      1
    ## BPQ150C       1        1        1       1       1       0       1      1      1
    ##          BPXPLS BPXPULS BPXPTY BPXML1 BPXSY1 BPXDI1 BPXSY2 BPXDI2 BPAEN2 BPXSY3
    ## HSD010        1       1      1      1      1      1      1      1      1      1
    ## PEASCST1      1       1      1      1      1      1      1      1      1      1
    ## PEASCTM1      1       1      1      1      1      1      1      1      1      1
    ## BPQ150A       1       1      1      1      1      1      1      1      1      1
    ## BPQ150B       1       1      1      1      1      1      1      1      1      1
    ## BPQ150C       1       1      1      1      1      1      1      1      1      1
    ##          BPXDI3 BPAEN3 LBXWBCSI LBXLYPCT LBXMOPCT LBXNEPCT LBXEOPCT LBXBAPCT
    ## HSD010        1      1        1        1        1        1        1        1
    ## PEASCST1      1      1        1        1        1        1        1        1
    ## PEASCTM1      1      1        1        1        1        1        1        1
    ## BPQ150A       1      1        1        1        1        1        1        1
    ## BPQ150B       1      1        1        1        1        1        1        1
    ## BPQ150C       1      1        1        1        1        1        1        1
    ##          LBDLYMNO LBDMONO LBDNENO LBDEONO LBDBANO LBXRBCSI LBXHGB LBXHCT
    ## HSD010          1       1       1       1       1        1      1      1
    ## PEASCST1        1       1       1       1       1        1      1      1
    ## PEASCTM1        1       1       1       1       1        1      1      1
    ## BPQ150A         1       1       1       1       1        1      1      1
    ## BPQ150B         1       1       1       1       1        1      1      1
    ## BPQ150C         1       1       1       1       1        1      1      1
    ##          LBXMCVSI LBXMCHSI LBXMC LBXRDW LBXPLTSI LBXMPSI HSD010_re
    ## HSD010          1        1     1      1        1       1         1
    ## PEASCST1        1        1     1      1        1       1         1
    ## PEASCTM1        1        1     1      1        1       1         1
    ## BPQ150A         1        1     1      1        1       1         1
    ## BPQ150B         1        1     1      1        1       1         1
    ## BPQ150C         1        1     1      1        1       1         1
    ## Number of logged events:  560 
    ##   it im      dep meth       out
    ## 1  1  1 PEASCTM1   rf PEASCST13
    ## 2  1  1  BPQ150A   rf PEASCST13
    ## 3  1  1  BPQ150B   rf PEASCST13
    ## 4  1  1  BPQ150C   rf PEASCST13
    ## 5  1  1  BPQ150D   rf PEASCST13
    ## 6  1  1   BPAARM   rf PEASCST13

    mice.df <- complete(mice.mis)  # generate the completed data.
    anyNA(mice.df)

    ## [1] FALSE

There are no remaining missing values in the dataset.

### Unbalanced data split:

Common ways to deal with unbalanced data is to undersample or oversample
respective underreprsented or overrepresented classes. That might not be
always the best solution, and K means algoruthm is sometimes used to
deal with the unbalanced. Kmeans SMOTE (Synthetic Minority Over-sampling
Technique) is one of such methods - can be implemented in Python -
synthetic dataset is created to make up for the imbalance in the smaller
class, as the cost of not observing a smaller interetsting class can be
significant ( often those are the cases we want to
identify).\[<a href="https://jair.org/index.php/jair/article/view/10302" class="uri">https://jair.org/index.php/jair/article/view/10302</a>\].
Library ROSE in R also generates synthetic observations and allows us to
not loose significant data from undersampling, while minimizing the
issue of repeating the same vaues in overampling the smaller class. Is
is also cited that undersampling the majority can achieve a better
classifier performace. I am going to attempt that on my data set:

    library(ROSE)
    # a repeated column:
    mice.df<- mice.df %>% dplyr::select(-HSD010)
    # perform synthetic data additon 
    mice.df.rose <- ROSE(HSD010_re ~ ., data = mice.df, seed = 1)$data
    # this generated a well balanced dataset. 
    prop.table(table(mice.df.rose$HSD010_re))

    ## 
    ##         1         0 
    ## 0.5044206 0.4955794

Next, I want to see if this will make my classifiers better:

    # Dimensions are the same, I can use the same indexing for splitting the data: 
    dim(mice.df.rose)

    ## [1] 6334   41

    mice.rose.train<- mice.df.rose[trainIndex, ]
    mice.rose.test<- mice.df.rose[-trainIndex, ]
    mice.rose.test.y<- mice.rose.test %>% dplyr::select(HSD010_re)
    mice.rose.test<- mice.rose.test %>% dplyr::select(-HSD010_re)

#### Logistic regression in mice rose:

    glm.m2<- glm(HSD010_re~., data = mice.rose.train, family = "binomial")
    summary(glm.m2)

    ## 
    ## Call:
    ## glm(formula = HSD010_re ~ ., family = "binomial", data = mice.rose.train)
    ## 
    ## Deviance Residuals: 
    ##     Min       1Q   Median       3Q      Max  
    ## -2.1166  -1.0875  -0.6057   1.1340   1.9913  
    ## 
    ## Coefficients:
    ##              Estimate Std. Error z value Pr(>|z|)    
    ## (Intercept) -0.045013   1.352393  -0.033 0.973448    
    ## PEASCST12   -0.617190   1.174624  -0.525 0.599280    
    ## PEASCST13   -0.145757   0.236101  -0.617 0.537004    
    ## PEASCTM1    -0.023733   0.027693  -0.857 0.391442    
    ## BPQ150A2     0.194963   0.073035   2.669 0.007598 ** 
    ## BPQ150B2    -1.382321   1.125709  -1.228 0.219464    
    ## BPQ150C2    -0.586679   0.321662  -1.824 0.068167 .  
    ## BPQ150D2    -0.359072   0.199022  -1.804 0.071202 .  
    ## BPAARM2      1.015369   0.467446   2.172 0.029843 *  
    ## BPAARM8      1.209935   1.494544   0.810 0.418188    
    ## BPACSZ3     -0.024784   0.290412  -0.085 0.931990    
    ## BPACSZ4      0.090045   0.288563   0.312 0.755006    
    ## BPACSZ5      0.593931   0.296022   2.006 0.044816 *  
    ## BPXPLS       0.046629   0.024704   1.888 0.059086 .  
    ## BPXPULS2     0.144652   0.195830   0.739 0.460115    
    ## BPXPTY2      1.500110   0.831214   1.805 0.071118 .  
    ## BPXML1       0.037198   0.031831   1.169 0.242568    
    ## BPXSY1       0.120081   0.032644   3.679 0.000235 ***
    ## BPXDI1      -0.033783   0.034540  -0.978 0.328035    
    ## BPXSY2       0.051789   0.033825   1.531 0.125748    
    ## BPXDI2      -0.040673   0.035309  -1.152 0.249345    
    ## BPAEN22      1.191798   0.379403   3.141 0.001682 ** 
    ## BPXSY3       0.072990   0.032713   2.231 0.025669 *  
    ## BPXDI3       0.030789   0.035193   0.875 0.381646    
    ## BPAEN32      0.812698   0.485772   1.673 0.094326 .  
    ## LBXWBCSI     0.081357   0.034670   2.347 0.018946 *  
    ## LBXLYPCT    -0.018630   0.032412  -0.575 0.565431    
    ## LBXMOPCT    -0.035019   0.028845  -1.214 0.224725    
    ## LBXNEPCT     0.034839   0.033280   1.047 0.295175    
    ## LBXEOPCT     0.028915   0.031312   0.923 0.355776    
    ## LBXBAPCT    -0.028852   0.027601  -1.045 0.295883    
    ## LBDLYMNO     0.053349   0.038964   1.369 0.170942    
    ## LBDMONO     -0.042593   0.031039  -1.372 0.169989    
    ## LBDNENO      0.056667   0.033468   1.693 0.090416 .  
    ## LBDEONO      0.043265   0.030728   1.408 0.159126    
    ## LBDBANO     -0.003041   0.028813  -0.106 0.915958    
    ## LBXRBCSI    -0.020382   0.033170  -0.614 0.538914    
    ## LBXHGB      -0.092703   0.034286  -2.704 0.006856 ** 
    ## LBXHCT      -0.006970   0.033838  -0.206 0.836817    
    ## LBXMCVSI     0.019518   0.031493   0.620 0.535421    
    ## LBXMCHSI     0.031473   0.034440   0.914 0.360806    
    ## LBXMC       -0.057702   0.028010  -2.060 0.039397 *  
    ## LBXRDW       0.151907   0.026195   5.799 6.67e-09 ***
    ## LBXPLTSI    -0.021536   0.026071  -0.826 0.408779    
    ## LBXMPSI      0.026007   0.025319   1.027 0.304352    
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 6585.6  on 4750  degrees of freedom
    ## Residual deviance: 6209.3  on 4706  degrees of freedom
    ## AIC: 6299.3
    ## 
    ## Number of Fisher Scoring iterations: 4

performing similar step wise rergession:

    set.seed(123)
    best.glm.m2<- glm(HSD010_re~., data = mice.rose.train, family = "binomial") %>%
      stepAIC(trace = FALSE)
    summary(best.glm.m2)

    ## 
    ## Call:
    ## glm(formula = HSD010_re ~ BPQ150A + BPQ150C + BPQ150D + BPAARM + 
    ##     BPACSZ + BPXPLS + BPXPTY + BPXSY1 + BPXSY2 + BPXDI2 + BPAEN2 + 
    ##     BPXSY3 + BPAEN3 + LBXWBCSI + LBDMONO + LBDNENO + LBDEONO + 
    ##     LBXHGB + LBXMCHSI + LBXMC + LBXRDW, family = "binomial", 
    ##     data = mice.rose.train)
    ## 
    ## Deviance Residuals: 
    ##     Min       1Q   Median       3Q      Max  
    ## -2.0921  -1.0952  -0.6182   1.1431   2.0283  
    ## 
    ## Coefficients:
    ##               Estimate Std. Error z value Pr(>|z|)    
    ## (Intercept) -1.4860958  0.7449918  -1.995  0.04607 *  
    ## BPQ150A2     0.1934764  0.0725723   2.666  0.00768 ** 
    ## BPQ150C2    -0.5923961  0.3205140  -1.848  0.06456 .  
    ## BPQ150D2    -0.3673539  0.1975169  -1.860  0.06291 .  
    ## BPAARM2      1.0313546  0.4649459   2.218  0.02654 *  
    ## BPAARM8      0.6609165  0.9218668   0.717  0.47342    
    ## BPACSZ3      0.0002802  0.2883196   0.001  0.99922    
    ## BPACSZ4      0.1043520  0.2863603   0.364  0.71555    
    ## BPACSZ5      0.5975370  0.2933548   2.037  0.04166 *  
    ## BPXPLS       0.0431240  0.0243528   1.771  0.07659 .  
    ## BPXPTY2      1.5015164  0.8296435   1.810  0.07032 .  
    ## BPXSY1       0.1297019  0.0313324   4.140 3.48e-05 ***
    ## BPXSY2       0.0621110  0.0326154   1.904  0.05687 .  
    ## BPXDI2      -0.0417097  0.0285493  -1.461  0.14402    
    ## BPAEN22      1.2228585  0.3782439   3.233  0.00123 ** 
    ## BPXSY3       0.0815280  0.0318454   2.560  0.01046 *  
    ## BPAEN32      0.8434492  0.4813110   1.752  0.07971 .  
    ## LBXWBCSI     0.0974517  0.0325838   2.991  0.00278 ** 
    ## LBDMONO     -0.0591023  0.0274125  -2.156  0.03108 *  
    ## LBDNENO      0.0830710  0.0290494   2.860  0.00424 ** 
    ## LBDEONO      0.0568049  0.0244508   2.323  0.02017 *  
    ## LBXHGB      -0.1018937  0.0261219  -3.901 9.59e-05 ***
    ## LBXMCHSI     0.0528964  0.0281985   1.876  0.06068 .  
    ## LBXMC       -0.0561142  0.0275379  -2.038  0.04158 *  
    ## LBXRDW       0.1523930  0.0258159   5.903 3.57e-09 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 6585.6  on 4750  degrees of freedom
    ## Residual deviance: 6226.5  on 4726  degrees of freedom
    ## AIC: 6276.5
    ## 
    ## Number of Fisher Scoring iterations: 4

Variance inflation factor exploration:

    vif(best.glm.m2)

    ##              GVIF Df GVIF^(1/(2*Df))
    ## BPQ150A  1.008046  1        1.004015
    ## BPQ150C  1.022818  1        1.011345
    ## BPQ150D  1.035559  1        1.017624
    ## BPAARM   1.012257  2        1.003050
    ## BPACSZ   1.077480  3        1.012515
    ## BPXPLS   1.042779  1        1.021166
    ## BPXPTY   1.004546  1        1.002270
    ## BPXSY1   1.770101  1        1.330451
    ## BPXSY2   1.885731  1        1.373219
    ## BPXDI2   1.122538  1        1.059499
    ## BPAEN2   1.012634  1        1.006297
    ## BPXSY3   1.788150  1        1.337217
    ## BPAEN3   1.010054  1        1.005014
    ## LBXWBCSI 1.540624  1        1.241219
    ## LBDMONO  1.144122  1        1.069636
    ## LBDNENO  1.448733  1        1.203633
    ## LBDEONO  1.038380  1        1.019009
    ## LBXHGB   1.175495  1        1.084202
    ## LBXMCHSI 1.342744  1        1.158768
    ## LBXMC    1.282954  1        1.132675
    ## LBXRDW   1.152174  1        1.073394

No significant issues identified there.

We can perform additional formula tweaking for this model, as there are
still features used that are non significant. Previous step wise
regression was done on the basis of AIC. Next can be done on the basis
of p value of each included factor. But first, I am going to test how
well this model performs:

    probabilities_2 <- best.glm.m2 %>% predict(mice.rose.test, type = "response")
    predicted.classes_2 <- ifelse(probabilities_2 > 0.5, 1, 0)
    mean(predicted.classes_2 == mice.rose.test.y$HSD010_re)

    ## [1] 0.3954517

    pre.df_2<- as.data.frame(predicted.classes_2)
    pre.df_2$predicted.classes_2<- as.factor(pre.df_2$predicted.classes_2)
    confusionMatrix(pre.df_2$predicted.classes_2, mice.rose.test.y$HSD010_re)

    ## Warning in confusionMatrix.default(pre.df_2$predicted.classes_2,
    ## mice.rose.test.y$HSD010_re): Levels are not in the same order for reference and
    ## data. Refactoring data to match.

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   1   0
    ##          1 272 438
    ##          0 519 354
    ##                                         
    ##                Accuracy : 0.3955        
    ##                  95% CI : (0.3713, 0.42)
    ##     No Information Rate : 0.5003        
    ##     P-Value [Acc > NIR] : 1.000000      
    ##                                         
    ##                   Kappa : -0.2092       
    ##                                         
    ##  Mcnemar's Test P-Value : 0.009709      
    ##                                         
    ##             Sensitivity : 0.3439        
    ##             Specificity : 0.4470        
    ##          Pos Pred Value : 0.3831        
    ##          Neg Pred Value : 0.4055        
    ##              Prevalence : 0.4997        
    ##          Detection Rate : 0.1718        
    ##    Detection Prevalence : 0.4485        
    ##       Balanced Accuracy : 0.3954        
    ##                                         
    ##        'Positive' Class : 1             
    ## 

Upon this modification, Sensitivity was significantly increased,
however, the overall accuracy of the model suffered. In general, less
than half predictions are correct.

    library(pROC)
    pre.df_2$predicted.classes<- as.numeric(pre.df_2$predicted.classes)
    mice.rose.test.y$HSD010_re_int<- as.numeric(mice.rose.test.y$HSD010_re)
    roccurve.glm_2 <- roc(mice.rose.test.y$HSD010_re_int ~ pre.df_2$predicted.classes)

    ## Setting levels: control = 1, case = 2

    ## Setting direction: controls < cases

    plot(roccurve.glm_2)

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-57-1.png)

    cat("Area under the curve",auc(roccurve.glm_2), "shows some improvement in the model")

    ## Area under the curve 0.6045809 shows some improvement in the model

##### K fold cross validation - GLM

Logistic regression has no hyperparameters to tune over; the estimates
for the coefficients will always be given by maximum likelihood. The
repeated k-fold cross validation will do nothing to affect the estimates
of the
(parameters/coefficients)\[<a href="https://stats.stackexchange.com/questions/280533/should-logistic-regression-models-generated-with-and-without-cross-validation-in" class="uri">https://stats.stackexchange.com/questions/280533/should-logistic-regression-models-generated-with-and-without-cross-validation-in</a>\]

    best.glm.m2$coefficients

    ##  (Intercept)     BPQ150A2     BPQ150C2     BPQ150D2      BPAARM2      BPAARM8 
    ## -1.486095777  0.193476436 -0.592396128 -0.367353890  1.031354552  0.660916484 
    ##      BPACSZ3      BPACSZ4      BPACSZ5       BPXPLS      BPXPTY2       BPXSY1 
    ##  0.000280216  0.104351969  0.597537035  0.043123994  1.501516435  0.129701915 
    ##       BPXSY2       BPXDI2      BPAEN22       BPXSY3      BPAEN32     LBXWBCSI 
    ##  0.062111024 -0.041709682  1.222858457  0.081528044  0.843449242  0.097451740 
    ##      LBDMONO      LBDNENO      LBDEONO       LBXHGB     LBXMCHSI        LBXMC 
    ## -0.059102303  0.083070977  0.056804929 -0.101893688  0.052896402 -0.056114241 
    ##       LBXRDW 
    ##  0.152393029

    set.seed(123)
    fit.control <- caret::trainControl(method = "cv", number = 10)

    glm.fit <- caret::train(HSD010_re ~  BPQ150A  +  BPQ150C  +  BPQ150D  +   BPAARM   +  BPAARM   +  BPACSZ+     BPACSZ  +   BPACSZ +BPXPLS +    BPXPTY  +    BPXML1   +   BPXSY1     +BPAEN2 +     BPXSY3  +   BPAEN3   + LBXWBCSI  +  LBXNEPCT+ LBXBAPCT + LBDMONO  +   LBDNENO  +   LBDEONO   +   LBXHGB  +  LBXMCHSI   +    LBXMC  +    LBXRDW ,
                           data = mice.rose.train,
                           method = "glm",
                           trControl = fit.control)
    print(glm.fit)

    ## Generalized Linear Model 
    ## 
    ## 4751 samples
    ##   22 predictor
    ##    2 classes: '1', '0' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold) 
    ## Summary of sample sizes: 4276, 4275, 4275, 4277, 4275, 4277, ... 
    ## Resampling results:
    ## 
    ##   Accuracy   Kappa    
    ##   0.6036575  0.2061733

Using this “best” chosen model through k fold cross validation, we can
make the following prediction:

    predictions.glm<-as.data.frame(predict.train(object= glm.fit,mice.rose.test,type="raw"))
    names(predictions.glm)<- "pred"

    confusionMatrix(predictions.glm$pred, as.factor(mice.rose.test.y$HSD010_re))

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   1   0
    ##          1 516 335
    ##          0 275 457
    ##                                           
    ##                Accuracy : 0.6147          
    ##                  95% CI : (0.5902, 0.6387)
    ##     No Information Rate : 0.5003          
    ##     P-Value [Acc > NIR] : <2e-16          
    ##                                           
    ##                   Kappa : 0.2293          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.0169          
    ##                                           
    ##             Sensitivity : 0.6523          
    ##             Specificity : 0.5770          
    ##          Pos Pred Value : 0.6063          
    ##          Neg Pred Value : 0.6243          
    ##              Prevalence : 0.4997          
    ##          Detection Rate : 0.3260          
    ##    Detection Prevalence : 0.5376          
    ##       Balanced Accuracy : 0.6147          
    ##                                           
    ##        'Positive' Class : 1               
    ## 

#### Rules classification

    set.seed(123)
    mice.rose.rules.train_y<- mice.rose.train %>% dplyr::select(HSD010_re)
    mice.rose.rules.train<- mice.rose.train %>% dplyr::select(-HSD010_re)

    rules_micerose_mod <- C5.0(x = mice.rose.rules.train, y = mice.rose.rules.train_y$HSD010_re, rules = TRUE)
    summary(rules_micerose_mod)

    ## 
    ## Call:
    ## C5.0.default(x = mice.rose.rules.train, y =
    ##  mice.rose.rules.train_y$HSD010_re, rules = TRUE)
    ## 
    ## 
    ## C5.0 [Release 2.07 GPL Edition]      Sun Aug 23 12:15:29 2020
    ## -------------------------------
    ## 
    ## Class specified by attribute `outcome'
    ## 
    ## Read 4751 cases (41 attributes) from undefined.data
    ## 
    ## Rules:
    ## 
    ## Rule 1: (51/9, lift 1.6)
    ##  BPAEN2 = 1
    ##  ->  class 1  [0.811]
    ## 
    ## Rule 2: (1674/523, lift 1.4)
    ##  BPAARM = 1
    ##  BPACSZ in {3, 4}
    ##  BPXPULS = 1
    ##  BPXPTY = 1
    ##  BPXML1 <= 0.880964
    ##  BPXSY2 <= 0.9231525
    ##  BPXSY3 <= 1.888162
    ##  LBXNEPCT <= 2.329015
    ##  LBDBANO > -1.881295
    ##  LBDBANO <= 2.005875
    ##  LBXRDW > -1.771649
    ##  LBXRDW <= 0.6602584
    ##  ->  class 1  [0.687]
    ## 
    ## Rule 3: (740/237, lift 1.3)
    ##  BPAARM = 1
    ##  BPACSZ = 3
    ##  BPXPULS = 1
    ##  BPXSY1 <= 0.6480333
    ##  BPXSY2 <= 1.378618
    ##  BPXDI3 <= 1.449384
    ##  LBXNEPCT <= 1.178581
    ##  LBDBANO > -1.881295
    ##  ->  class 1  [0.679]
    ## 
    ## Rule 4: (4700/2338, lift 1.0)
    ##  BPAEN2 = 2
    ##  ->  class 1  [0.503]
    ## 
    ## Rule 5: (22, lift 1.9)
    ##  BPACSZ in {3, 4}
    ##  BPXML1 > 0.880964
    ##  BPXSY2 <= 0.9231525
    ##  LBXLYPCT > 1.793088
    ##  LBDBANO > -1.881295
    ##  LBXRDW > -1.771649
    ##  ->  class 0  [0.958]
    ## 
    ## Rule 6: (18, lift 1.9)
    ##  BPQ150D = 1
    ##  BPACSZ in {3, 4}
    ##  BPXSY2 > 0.9231525
    ##  LBDBANO <= 2.005875
    ##  LBXRDW > -1.771649
    ##  ->  class 0  [0.950]
    ## 
    ## Rule 7: (37/2, lift 1.9)
    ##  BPACSZ in {3, 4}
    ##  BPXML1 <= -0.8045264
    ##  BPXSY2 > 0.9231525
    ##  LBXRDW > -1.771649
    ##  ->  class 0  [0.923]
    ## 
    ## Rule 8: (43/4, lift 1.8)
    ##  BPACSZ = 5
    ##  BPXSY3 > 1.888162
    ##  BPXDI3 > 0.1141533
    ##  LBDBANO > -1.881295
    ##  LBXMCHSI <= 1.347164
    ##  ->  class 0  [0.889]
    ## 
    ## Rule 9: (23/3, lift 1.7)
    ##  BPXSY3 > 1.888162
    ##  BPXDI3 <= 0.1141533
    ##  LBXHGB > 0.6803019
    ##  LBXRDW <= 0.6602584
    ##  ->  class 0  [0.840]
    ## 
    ## Rule 10: (50/8, lift 1.7)
    ##  BPXPULS = 2
    ##  BPXSY1 <= -0.008790038
    ##  ->  class 0  [0.827]
    ## 
    ## Rule 11: (336/61, lift 1.7)
    ##  BPXPULS = 1
    ##  BPXSY1 > 0.6480333
    ##  BPAEN2 = 2
    ##  LBDNENO > -1.163644
    ##  LBXRDW > 0.6602584
    ##  ->  class 0  [0.817]
    ## 
    ## Rule 12: (73/13, lift 1.6)
    ##  BPACSZ = 3
    ##  BPXSY3 > 1.888162
    ##  LBXMC <= 0.3206452
    ##  ->  class 0  [0.813]
    ## 
    ## Rule 13: (235/45, lift 1.6)
    ##  BPAEN2 = 2
    ##  LBXNEPCT > 1.178581
    ##  LBXRDW > 0.6602584
    ##  ->  class 0  [0.806]
    ## 
    ## Rule 14: (181/36, lift 1.6)
    ##  LBDBANO <= -1.881295
    ##  ->  class 0  [0.798]
    ## 
    ## Rule 15: (31/6, lift 1.6)
    ##  BPAARM = 2
    ##  ->  class 0  [0.788]
    ## 
    ## Rule 16: (148/31, lift 1.6)
    ##  PEASCST1 = 1
    ##  LBXBAPCT <= 2.161297
    ##  LBDBANO > 2.005875
    ##  LBXRDW > -1.771649
    ##  ->  class 0  [0.787]
    ## 
    ## Rule 17: (265/57, lift 1.6)
    ##  BPACSZ in {2, 5}
    ##  BPAEN2 = 2
    ##  LBDBANO > -1.881295
    ##  LBXRDW > 0.6602584
    ##  ->  class 0  [0.783]
    ## 
    ## Rule 18: (136/30, lift 1.6)
    ##  LBXNEPCT > 2.329015
    ##  ->  class 0  [0.775]
    ## 
    ## Rule 19: (8/2, lift 1.4)
    ##  BPXPTY = 2
    ##  ->  class 0  [0.700]
    ## 
    ## Rule 20: (188/62, lift 1.4)
    ##  LBXRDW <= -1.771649
    ##  ->  class 0  [0.668]
    ## 
    ## Rule 21: (1659/780, lift 1.1)
    ##  LBXHGB <= -0.512726
    ##  ->  class 0  [0.530]
    ## 
    ## Default class: 1
    ## 
    ## 
    ## Evaluation on training data (4751 cases):
    ## 
    ##          Rules     
    ##    ----------------
    ##      No      Errors
    ## 
    ##      21 1577(33.2%)   <<
    ## 
    ## 
    ##     (a)   (b)    <-classified as
    ##    ----  ----
    ##    1883   521    (a): class 1
    ##    1056  1291    (b): class 0
    ## 
    ## 
    ##  Attribute usage:
    ## 
    ##  100.00% BPAEN2
    ##   56.54% LBXRDW
    ##   54.03% LBDBANO
    ##   49.93% BPACSZ
    ##   48.94% BPXPULS
    ##   47.59% LBXNEPCT
    ##   42.31% BPXSY2
    ##   41.46% BPAARM
    ##   38.08% BPXSY3
    ##   36.48% BPXML1
    ##   35.40% BPXPTY
    ##   35.40% LBXHGB
    ##   23.70% BPXSY1
    ##   16.96% BPXDI3
    ##    7.07% LBDNENO
    ##    3.12% PEASCST1
    ##    3.12% LBXBAPCT
    ##    1.54% LBXMC
    ##    0.91% LBXMCHSI
    ##    0.46% LBXLYPCT
    ##    0.38% BPQ150D
    ## 
    ## 
    ## Time: 0.3 secs

    rules.pred_2<- predict(rules_micerose_mod, newdata = mice.rose.test)
    rules.pred_2<- as.data.frame(rules.pred_2)
    rules.pred_2$rules.pred_2<- as.factor(rules.pred_2$rules.pred_2)
    mice.rose.test.y$HSD010_re<- as.factor(mice.rose.test.y$HSD010_re)
    confusionMatrix(rules.pred_2$rules.pred_2, mice.rose.test.y$HSD010_re)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   1   0
    ##          1 570 373
    ##          0 221 419
    ##                                           
    ##                Accuracy : 0.6248          
    ##                  95% CI : (0.6004, 0.6487)
    ##     No Information Rate : 0.5003          
    ##     P-Value [Acc > NIR] : < 2.2e-16       
    ##                                           
    ##                   Kappa : 0.2496          
    ##                                           
    ##  Mcnemar's Test P-Value : 5.806e-10       
    ##                                           
    ##             Sensitivity : 0.7206          
    ##             Specificity : 0.5290          
    ##          Pos Pred Value : 0.6045          
    ##          Neg Pred Value : 0.6547          
    ##              Prevalence : 0.4997          
    ##          Detection Rate : 0.3601          
    ##    Detection Prevalence : 0.5957          
    ##       Balanced Accuracy : 0.6248          
    ##                                           
    ##        'Positive' Class : 1               
    ## 

Rules perform better on this model than lofistic regression. Sensitivity
was significantly increased compared to the non mice imputed and ROSE
synthetically generated data using model. Accuracy is jsut above a half,
so our prediction with this model would be somewhat better than chance.

    rules.pred_2$rules.pred_2_inte<- as.numeric(rules.pred_2$rules.pred_2)
    roccurve.rules_2 <- roc(mice.rose.test.y$HSD010_re_int ~ pre.df_2$predicted.classes)

    ## Setting levels: control = 1, case = 2

    ## Setting direction: controls < cases

    plot(roccurve.rules_2)

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-64-1.png)

    auc(roccurve.rules_2)

    ## Area under the curve: 0.6046

##### K fold cross validation:;

    set.seed(123)
    C5.fit <- caret::train(HSD010_re ~.,
                           data = mice.rose.train,
                           method = "C5.0",
                           trControl = fit.control, trace = FALSE)
    print(C5.fit)

    ## C5.0 
    ## 
    ## 4751 samples
    ##   40 predictor
    ##    2 classes: '1', '0' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold) 
    ## Summary of sample sizes: 4276, 4275, 4275, 4277, 4275, 4277, ... 
    ## Resampling results across tuning parameters:
    ## 
    ##   model  winnow  trials  Accuracy   Kappa    
    ##   rules  FALSE    1      0.6177739  0.2336266
    ##   rules  FALSE   10      0.6282821  0.2558060
    ##   rules  FALSE   20      0.6282821  0.2558060
    ##   rules   TRUE    1      0.6207115  0.2394694
    ##   rules   TRUE   10      0.6211197  0.2411844
    ##   rules   TRUE   20      0.6211197  0.2411844
    ##   tree   FALSE    1      0.6085027  0.2151473
    ##   tree   FALSE   10      0.6308022  0.2608883
    ##   tree   FALSE   20      0.6308022  0.2608883
    ##   tree    TRUE    1      0.6143921  0.2275507
    ##   tree    TRUE   10      0.6255483  0.2501748
    ##   tree    TRUE   20      0.6255483  0.2501748
    ## 
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final values used for the model were trials = 10, model = tree and winnow
    ##  = FALSE.

This allows us to explore the relatoinship between the estimates of the
performance and the tunning parameters used here: this demonstrates that
the oevrall accuracy of the rules and the tree is higher within the no
Winnowing tunning.

    trellis.par.set(caretTheme())
    plot(C5.fit)  

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-67-1.png)

Prediction using k fold cross validated model:

    predictions.C5<- data.frame(pred = predict.train(C5.fit, mice.rose.test))
    confusionMatrix(predictions.C5$pred, mice.rose.test.y$HSD010_re)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   1   0
    ##          1 517 301
    ##          0 274 491
    ##                                           
    ##                Accuracy : 0.6368          
    ##                  95% CI : (0.6125, 0.6605)
    ##     No Information Rate : 0.5003          
    ##     P-Value [Acc > NIR] : <2e-16          
    ##                                           
    ##                   Kappa : 0.2735          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.2782          
    ##                                           
    ##             Sensitivity : 0.6536          
    ##             Specificity : 0.6199          
    ##          Pos Pred Value : 0.6320          
    ##          Neg Pred Value : 0.6418          
    ##              Prevalence : 0.4997          
    ##          Detection Rate : 0.3266          
    ##    Detection Prevalence : 0.5167          
    ##       Balanced Accuracy : 0.6368          
    ##                                           
    ##        'Positive' Class : 1               
    ## 

Using k fold cross validation, we can obtain a well balanced model that
brings up the values of sensitivity and specificity, while maintaing
some favorable accuracy. While as we can see from the ensemble built,
this k fold cross vaidated rule model is not as good as the full
ensemble, there is still some value to it.

#### Neural network:

    micerose.df.dummy<- fastDummies::dummy_columns(mice.df.rose,
    select_columns= c("PEASCST1",  "BPQ150A" ,  "BPQ150B" ,  "BPQ150C"  ,
                      "BPQ150D",  "BPAARM"  , "BPACSZ", "BPXPULS", 
                      "BPXPTY" ,    "BPAEN2" ,   "BPAEN3"),
    remove_first_dummy = T,
    remove_selected_columns = T)

    NN_train_2<- micerose.df.dummy[trainIndex, ]
    NN_test_2<- micerose.df.dummy[-trainIndex, ]
    NN_test_2_y<- NN_test_2 %>% dplyr::select(HSD010_re)
    NN_test_2_y$HSD010_re<- ifelse(NN_test_2_y$HSD010_re == 1, 1, 0)
    NN_test_2 <- NN_test_2 %>% dplyr::select(-HSD010_re)
    NN_train_2$HSD010_re<- ifelse(NN_train_2$HSD010_re==1, 1, 0)

There is no need to add additional hidden layers to the model as the
data is not complex enough to significanlty benefit from it.

    set.seed(123)
    # linear.output=FALSE specifies that I want to do classification and not regression. 
    neu.class2<- neuralnet(f, data = NN_train_2, linear.output= FALSE, stepmax=1e6)

    plot(neu.class2, rep = "best")

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-70-1.png)

The correlation between the predicted values and the actual out of
sample test response variable is quite low.

    neu.class2.pred<- compute(neu.class2, NN_test_2)
    cor(NN_test_2_y$HSD010_re, neu.class2.pred$net.result)

    ##           [,1]
    ## [1,] 0.1587883

Preparing the data for the confusion Matrix.

    prob <- neu.class2.pred$net.result
    pred <- ifelse(prob>0.5, 1, 0)
    pred<- as.data.frame(pred)
    pred$V1<- as.factor(pred$V1)
    NN_test_2_y$y_fac<-as.factor(NN_test_2_y$HSD010_re)

    confusionMatrix(data = pred$V1, reference = NN_test_2_y$y_fac)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   0   1
    ##          0 285 172
    ##          1 507 619
    ##                                           
    ##                Accuracy : 0.5711          
    ##                  95% CI : (0.5463, 0.5956)
    ##     No Information Rate : 0.5003          
    ##     P-Value [Acc > NIR] : 9.882e-09       
    ##                                           
    ##                   Kappa : 0.1424          
    ##                                           
    ##  Mcnemar's Test P-Value : < 2.2e-16       
    ##                                           
    ##             Sensitivity : 0.3598          
    ##             Specificity : 0.7826          
    ##          Pos Pred Value : 0.6236          
    ##          Neg Pred Value : 0.5497          
    ##              Prevalence : 0.5003          
    ##          Detection Rate : 0.1800          
    ##    Detection Prevalence : 0.2887          
    ##       Balanced Accuracy : 0.5712          
    ##                                           
    ##        'Positive' Class : 0               
    ## 

This Neural net perfoms much better - better so than the original neural
net and still somwehat worse than the new set of rules on the mice rose
treated data set. The goal of increasing sensitivity was achieved,
which, arguably, is the more important factor. Obtaining a true positive
of 0 as in not having reported good health is the important
classification here. There is no value in prediction of the
classification if everybody is automatically considered healthy by
default. Still, sensitivity is below 0.5 - to applicable in real life
conditions.

Double checking the distributions of the data for neural net - it needs
to be normal - this condition is maintained:

    mice.df.rose %>% dplyr::select(-HSD010_re) %>% 
      select_if(is.numeric) %>%  gather(cols, value) %>% 
      ggplot(aes(x = value)) + geom_histogram() + 
      facet_wrap(.~cols, scale = "free") + 
      ggtitle("Distributions of the numeric\n values in the full blood dataset- \n  labs and pressure") +
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  + 
      theme(plot.title = element_text(hjust = 0.5))

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-74-1.png)

ROC and AUC:

    pred$V1<- as.integer(pred$V1)
    roccurve.neural_2 <- roc(NN_test_2_y$HSD010_re ~ pred$V1)

    ## Setting levels: control = 0, case = 1

    ## Setting direction: controls < cases

    plot(roccurve.neural_2)

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-75-1.png)

Consistently, however, the area under the curve remains not particularly
high and equal wihin the three classification models I used.

    auc(roccurve.neural_2)

    ## Area under the curve: 0.5712

##### K fold cross validation:

    set.seed(123)
    nn.fit <- caret::train(HSD010_re ~.,
                           data = mice.rose.train,
                           method = "nnet",
                           trControl = fit.control, trace = FALSE)

    print(nn.fit)

    ## Neural Network 
    ## 
    ## 4751 samples
    ##   40 predictor
    ##    2 classes: '1', '0' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold) 
    ## Summary of sample sizes: 4276, 4275, 4275, 4277, 4275, 4277, ... 
    ## Resampling results across tuning parameters:
    ## 
    ##   size  decay  Accuracy   Kappa    
    ##   1     0e+00  0.5834420  0.1623427
    ##   1     1e-04  0.5855540  0.1672449
    ##   1     1e-01  0.5939835  0.1843187
    ##   3     0e+00  0.5887163  0.1755416
    ##   3     1e-04  0.5874620  0.1734652
    ##   3     1e-01  0.6089264  0.2163538
    ##   5     0e+00  0.5982024  0.1950326
    ##   5     1e-04  0.5823898  0.1639965
    ##   5     1e-01  0.6044956  0.2081522
    ## 
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final values used for the model were size = 3 and decay = 0.1.

Using k fold cross validation, we obtain the best performing model with
accuracy just baout 60% - within the range of what was obtained before.

    predictions.nn<- data.frame(pred = predict.train(nn.fit, mice.rose.test))
    confusionMatrix(predictions.nn$pred, mice.rose.test.y$HSD010_re)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   1   0
    ##          1 503 324
    ##          0 288 468
    ##                                           
    ##                Accuracy : 0.6134          
    ##                  95% CI : (0.5889, 0.6375)
    ##     No Information Rate : 0.5003          
    ##     P-Value [Acc > NIR] : <2e-16          
    ##                                           
    ##                   Kappa : 0.2268          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.1571          
    ##                                           
    ##             Sensitivity : 0.6359          
    ##             Specificity : 0.5909          
    ##          Pos Pred Value : 0.6082          
    ##          Neg Pred Value : 0.6190          
    ##              Prevalence : 0.4997          
    ##          Detection Rate : 0.3178          
    ##    Detection Prevalence : 0.5224          
    ##       Balanced Accuracy : 0.6134          
    ##                                           
    ##        'Positive' Class : 1               
    ## 

#### SVM model:

Next, I am going to perform SVM with radial kernel on my mice imputed
and synthetically balanced dataset.

    set.seed(123)
    micerose.svm<- ksvm(HSD010_re~., data = mice.rose.train, kernel = "rbfdot")

    micerose.pred.svm<- predict(micerose.svm, newdata = mice.rose.test)


    micerose.pred.svm<- as.data.frame(micerose.pred.svm)
    confusionMatrix(micerose.pred.svm$micerose.pred.svm,mice.rose.test.y$HSD010_re )

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   1   0
    ##          1 583 332
    ##          0 208 460
    ##                                           
    ##                Accuracy : 0.6589          
    ##                  95% CI : (0.6349, 0.6822)
    ##     No Information Rate : 0.5003          
    ##     P-Value [Acc > NIR] : < 2.2e-16       
    ##                                           
    ##                   Kappa : 0.3178          
    ##                                           
    ##  Mcnemar's Test P-Value : 1.203e-07       
    ##                                           
    ##             Sensitivity : 0.7370          
    ##             Specificity : 0.5808          
    ##          Pos Pred Value : 0.6372          
    ##          Neg Pred Value : 0.6886          
    ##              Prevalence : 0.4997          
    ##          Detection Rate : 0.3683          
    ##    Detection Prevalence : 0.5780          
    ##       Balanced Accuracy : 0.6589          
    ##                                           
    ##        'Positive' Class : 1               
    ## 

Finaly, we can see a slightly more appropriate model for predicting the
reported health status 0 out overall accuracy is ~66%, sensitivity went
up from almost non existent to 72% and Specificity of about 60%. Overall
accuracy (65%) still outperforms Neural net classification in the
previous test.

    micerose.pred.svm$svm_pred<- ifelse(micerose.pred.svm$micerose.pred.svm == 0, 0, 1)
    roccurve_svm_2<- roc(mice.rose.test.y$HSD010_re, micerose.pred.svm$svm_pred)

    ## Setting levels: control = 1, case = 0

    ## Setting direction: controls > cases

    plot(roccurve_svm_2)

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-81-1.png)

Classification on this modified data set with SVM works the best out of
all previous models.

    cat("Area under the curve is ", auc(roccurve_svm_2), "which is an improvement to the previous algorithms used")

    ## Area under the curve is  0.6589249 which is an improvement to the previous algorithms used

##### K fold for SVM Radial:

    set.seed(123)
    svm.fit<- caret::train(HSD010_re~., data = mice.rose.train, method = "svmRadial",trControl = fit.control)
    print(svm.fit)

    ## Support Vector Machines with Radial Basis Function Kernel 
    ## 
    ## 4751 samples
    ##   40 predictor
    ##    2 classes: '1', '0' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold) 
    ## Summary of sample sizes: 4276, 4275, 4275, 4277, 4275, 4277, ... 
    ## Resampling results across tuning parameters:
    ## 
    ##   C     Accuracy   Kappa    
    ##   0.25  0.6219574  0.2424190
    ##   0.50  0.6402737  0.2792366
    ##   1.00  0.6592220  0.3173303
    ## 
    ## Tuning parameter 'sigma' was held constant at a value of 0.01632298
    ## Accuracy was used to select the optimal model using the largest value.
    ## The final values used for the model were sigma = 0.01632298 and C = 1.

With k fold cross validation, we otbain a slightly improved model in
terms of accuracy.

    set.seed(123)
    predictions.svm <- data.frame(pred= predict.train(svm.fit, mice.rose.test ))
    confusionMatrix(predictions.svm$pred, mice.rose.test.y$HSD010_re)

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   1   0
    ##          1 578 335
    ##          0 213 457
    ##                                           
    ##                Accuracy : 0.6538          
    ##                  95% CI : (0.6298, 0.6773)
    ##     No Information Rate : 0.5003          
    ##     P-Value [Acc > NIR] : < 2.2e-16       
    ##                                           
    ##                   Kappa : 0.3077          
    ##                                           
    ##  Mcnemar's Test P-Value : 2.355e-07       
    ##                                           
    ##             Sensitivity : 0.7307          
    ##             Specificity : 0.5770          
    ##          Pos Pred Value : 0.6331          
    ##          Neg Pred Value : 0.6821          
    ##              Prevalence : 0.4997          
    ##          Detection Rate : 0.3651          
    ##    Detection Prevalence : 0.5768          
    ##       Balanced Accuracy : 0.6539          
    ##                                           
    ##        'Positive' Class : 1               
    ## 

Radial kernel for SVM achievs the best results for classification.

Ensemble Building
-----------------

    library(caretEnsemble)
    set.seed(123)
    control <- trainControl(method="repeatedcv", number=10, classProbs=TRUE)

    algorithmList <- c('nnet', 'glm', 'C5.0', 'svmRadial')

    # try this to deal with factor level issue
    levels(mice.rose.train$HSD010_re) <- c("first_class", "second_class")
    levels(mice.rose.test.y$HSD010_re) <- c("first_class", "second_class")

    set.seed(345)
    models <- caretList(HSD010_re~., data=mice.rose.train, trControl=control, methodList=algorithmList, trace = FALSE)

    ## Warning in trControlCheck(x = trControl, y = target): trControl$savePredictions
    ## not 'all' or 'final'. Setting to 'final' so we can ensemble the models.

    ## Warning in trControlCheck(x = trControl, y = target): indexes not defined in
    ## trControl. Attempting to set them ourselves, so each model in the ensemble will
    ## have the same resampling indexes.

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 8 for this object. Predictions generated using 8
    ## trials

    ## Warning: 'trials' should be <= 8 for this object. Predictions generated using 8
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 9 for this object. Predictions generated using 9
    ## trials

    ## Warning: 'trials' should be <= 9 for this object. Predictions generated using 9
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 3 for this object. Predictions generated using 3
    ## trials

    ## Warning: 'trials' should be <= 3 for this object. Predictions generated using 3
    ## trials

    ## Warning: 'trials' should be <= 4 for this object. Predictions generated using 4
    ## trials

    ## Warning: 'trials' should be <= 4 for this object. Predictions generated using 4
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 8 for this object. Predictions generated using 8
    ## trials

    ## Warning: 'trials' should be <= 8 for this object. Predictions generated using 8
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 8 for this object. Predictions generated using 8
    ## trials

    ## Warning: 'trials' should be <= 8 for this object. Predictions generated using 8
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 3 for this object. Predictions generated using 3
    ## trials

    ## Warning: 'trials' should be <= 3 for this object. Predictions generated using 3
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 8 for this object. Predictions generated using 8
    ## trials

    ## Warning: 'trials' should be <= 8 for this object. Predictions generated using 8
    ## trials

    ## Warning: 'trials' should be <= 4 for this object. Predictions generated using 4
    ## trials

    ## Warning: 'trials' should be <= 4 for this object. Predictions generated using 4
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 4 for this object. Predictions generated using 4
    ## trials

    ## Warning: 'trials' should be <= 4 for this object. Predictions generated using 4
    ## trials

    ## Warning: 'trials' should be <= 4 for this object. Predictions generated using 4
    ## trials

    ## Warning: 'trials' should be <= 4 for this object. Predictions generated using 4
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 8 for this object. Predictions generated using 8
    ## trials

    ## Warning: 'trials' should be <= 8 for this object. Predictions generated using 8
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 6 for this object. Predictions generated using 6
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 5 for this object. Predictions generated using 5
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    ## Warning: 'trials' should be <= 7 for this object. Predictions generated using 7
    ## trials

    results <- resamples(models)
    summary(results)

    ## 
    ## Call:
    ## summary.resamples(object = results)
    ## 
    ## Models: nnet, glm, C5.0, svmRadial 
    ## Number of resamples: 10 
    ## 
    ## Accuracy 
    ##                Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
    ## nnet      0.5768421 0.5835510 0.6031579 0.5988121 0.6112594 0.6189474    0
    ## glm       0.5738397 0.5852632 0.5941132 0.5988103 0.6084211 0.6323529    0
    ## C5.0      0.5915789 0.6233558 0.6347368 0.6320709 0.6486090 0.6589474    0
    ## svmRadial 0.6294737 0.6473994 0.6589474 0.6623765 0.6836842 0.7010526    0
    ## 
    ## Kappa 
    ##                Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
    ## nnet      0.1512956 0.1650550 0.2050733 0.1962175 0.2220728 0.2361437    0
    ## glm       0.1463571 0.1692559 0.1870243 0.1966089 0.2162660 0.2643162    0
    ## C5.0      0.1797223 0.2450984 0.2677284 0.2627386 0.2968263 0.3180610    0
    ## svmRadial 0.2584050 0.2941508 0.3169712 0.3243223 0.3669912 0.4019330    0

    dotplot(results)

![](DA5030.Proj.Stoma_files/figure-markdown_strict/unnamed-chunk-85-1.png)
This plot demonstrates the obtained levels of accuracy and kappa (the
default metrics for classification algorithms) for all algorithms used
in the ensemble. This demonstrates that overall, similar to the results
obtained without k fold cross validation, logistic regression performs
with just about the worst accuracy, while svm with radila kernel does
the best in accuratly classifing our response variable.

**Stack**

Stack using simple logistic regression:

    set.seed(234)
    stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs=TRUE)
    stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl)
    print(stack.glm)

    ## A glm ensemble of 4 base models: nnet, glm, C5.0, svmRadial
    ## 
    ## Ensemble results:
    ## Generalized Linear Model 
    ## 
    ## 4751 samples
    ##    4 predictor
    ##    2 classes: 'first_class', 'second_class' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold, repeated 3 times) 
    ## Summary of sample sizes: 4276, 4276, 4276, 4277, 4275, 4277, ... 
    ## Resampling results:
    ## 
    ##   Accuracy   Kappa    
    ##   0.6679317  0.3354782

Doing similar stacking but checking for ROC, sensitivity, and
specificity of the same model. twoClassSummary uses AUC to compare the
models, therefore, this is the final model I am going to proceed with.

    set.seed(234)
    stackControl2<- trainControl(method="repeatedcv", number=10, repeats=3, classProbs=TRUE, summaryFunction=twoClassSummary)

    stack.glm2<- caretStack(models, method="glm", trControl=stackControl2)

    ## Warning in train.default(predobs$preds, predobs$obs, ...): The metric "Accuracy"
    ## was not in the result set. ROC will be used instead.

    print(stack.glm2)

    ## A glm ensemble of 4 base models: nnet, glm, C5.0, svmRadial
    ## 
    ## Ensemble results:
    ## Generalized Linear Model 
    ## 
    ## 4751 samples
    ##    4 predictor
    ##    2 classes: 'first_class', 'second_class' 
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold, repeated 3 times) 
    ## Summary of sample sizes: 4276, 4276, 4276, 4277, 4275, 4277, ... 
    ## Resampling results:
    ## 
    ##   ROC        Sens       Spec     
    ##   0.7254266  0.6901176  0.6452179

Upon using stakcing with simple logistic regression of the four models
used previosuly ( rules using C5.0 algorithm, neural net for
classification, logistic regression, and svm with radial kernel), I
achieved an accuracy level of 0.66 = ~67%. The same model evaluated with
ROC, sensitivity, and speicificity demonstrates ROC value of 0.72,
Sensitifvity of 69%, and specificity of about ~65%. These are all higher
characterstics than obtained through any inidividual model applied to
the imputed balanced dataset.

What ensemble model managed to accomplish is to develop a balanced
model- there is some accuracy loss overall, compared to, for example, k
fold validated svm radial model, but at the same time we have higher
sensitivity and specificity before.

Apply the stacked model:

    final.pred<-data.frame(pred = predict(stack.glm2,  mice.rose.test))
    confusionMatrix(final.pred$pred, mice.rose.test.y$HSD010_re)

    ## Confusion Matrix and Statistics
    ## 
    ##               Reference
    ## Prediction     first_class second_class
    ##   first_class          523          281
    ##   second_class         268          511
    ##                                           
    ##                Accuracy : 0.6532          
    ##                  95% CI : (0.6292, 0.6766)
    ##     No Information Rate : 0.5003          
    ##     P-Value [Acc > NIR] : <2e-16          
    ##                                           
    ##                   Kappa : 0.3064          
    ##                                           
    ##  Mcnemar's Test P-Value : 0.6085          
    ##                                           
    ##             Sensitivity : 0.6612          
    ##             Specificity : 0.6452          
    ##          Pos Pred Value : 0.6505          
    ##          Neg Pred Value : 0.6560          
    ##              Prevalence : 0.4997          
    ##          Detection Rate : 0.3304          
    ##    Detection Prevalence : 0.5079          
    ##       Balanced Accuracy : 0.6532          
    ##                                           
    ##        'Positive' Class : first_class     
    ## 

As expected from the analysis with k fold cross validation of the
stacked model, this model performs with 66% accuracy, sensitivity of
68%, and specificity of 63%. 95% confidence internval for the accuracy
woud be 0.6324, 0.6798. Overall, the model is significant. The classes
has to be recoded due to the neture of caretStack and classification
clagorithms but they were equally recoded in both train, test, and
reference.

Overall, we can succesfully predict the response of the health status of
the individual from blood lab results and related survey questions.
Although the prediction is not perfect, and the model performace is not
optimal, I can see potential for improvement with additional data - this
prediction is based from quite one sided blood results.

Analysis and CRISP DM
---------------------

Summarizing everything that was done in this project, I idenitified the
busines goal of performing this analysis - ideally, we would be able to
modify the health treatment plan to increase the number of people
reporting healthy status based on just limited and quite non invasive
lab work. Our data in this case was mixed - categorical and numerical,
representing survey results and actual blood lab results.The survey
results contained many missing values, which is a common issue with
survey data and in the data preparation point of the project, feature
elimination and data imputation was performed in order to use the data
the most effectively in the modelling. This work intersected with the
modelling work and some of the issues of imbalanced classes in the
original data became an issue upon the modelling itself - this requried
additional data preparation through synthetically generatting
underrepresented dataset. In terms of evaluation, there is no doubt that
the original dataset, although impouted with knn and mode, adn
transformed to be on the same scale and normalized, was not suitable for
utilization in classification using these algorithms. However, models
built on balanced data set had a significant improvement in their power.
AUC for the four models ( glm, rules, NN, SVM,) were the following:
0.6045809, 0.6046, 0.5712, 0.6589249; which clearly demostrates that SVM
with radial kernel has the best performance on this dataset. Overall,
the models perform pretty similarly on this new dataset, and have some
classification power. The caveats, as discussed , include disbalance in
the classes of the original dataset. Tunning of the model was done with
trainControl in the caret library.Deployment was not done in any way in
terms of Shiny app but everything was uploaded to Github:
<a href="https://github.com/valentinastoma/NationaHealth" class="uri">https://github.com/valentinastoma/NationaHealth</a>
